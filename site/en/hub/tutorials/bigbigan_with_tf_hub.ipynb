{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLOYL1PJAAtK"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fJWQ8WSAFhh"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1NTVIH6ABK-"
      },
      "source": [
        "# Generating Images with BigBiGAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/bigbigan_with_tf_hub\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/bigbigan_with_tf_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/hub/tutorials/bigbigan_with_tf_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/hub/tutorials/bigbigan_with_tf_hub.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/s?q=experts%2Fbert\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVvOoEhswyZg"
      },
      "source": [
        "This notebook is a demo for the *BigBiGAN* models available on [TF Hub](https://tfhub.dev/s?publisher=deepmind&q=bigbigan).\n",
        "\n",
        "BigBiGAN extends standard (Big)GANs by adding an *encoder* module which can be used for unsupervised representation learning. Roughly speaking, the encoder inverts the generator by predicting latents `z` given real data `x`. See the [BigBiGAN paper on arXiv](https://arxiv.org/abs/1907.02544) [1] for more information about these models.\n",
        "\n",
        "After connecting to a runtime, get started by following these instructions:\n",
        "\n",
        "1. (Optional) Update the selected **`module_path`** in the first code cell below to load a BigBiGAN generator for a different encoder architecture.\n",
        "2. Click **Runtime > Run all** to run each cell in order. Afterwards, the outputs, including visualizations of BigBiGAN samples and reconstructions, should automatically appear below.\n",
        "\n",
        "Note: if you run into any issues, it can help to click **Runtime > Restart and run all...** to restart your runtime and rerun all cells from scratch.\n",
        "\n",
        "[1] Jeff Donahue and Karen Simonyan. [Large Scale Adversarial Representation Learning](https://arxiv.org/abs/1907.02544). *arxiv:1907.02544*, 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtGFwUKOA9jt"
      },
      "source": [
        "First, set the module path.\n",
        "By default, we load the BigBiGAN model with the smaller ResNet-50-based encoder from  **`https://tfhub.dev/deepmind/bigbigan-resnet50/1`**.\n",
        "To load the larger RevNet-50-x4 based model used to achieve the best representation learning results, comment out the active **`module_path`** setting and uncomment the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoY9pl0FBoUS"
      },
      "outputs": [],
      "source": [
        "module_path = 'https://tfhub.dev/deepmind/bigbigan-resnet50/1'  # ResNet-50\n",
        "# module_path = 'https://tfhub.dev/deepmind/bigbigan-revnet50x4/1'  # RevNet-50 x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr01cszC_vcC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPdT-hYj1XXQ"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import IPython.display\n",
        "import PIL.Image\n",
        "from pprint import pformat\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouePZy6-CFJl"
      },
      "source": [
        "## Define some functions to display images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBQPtmrY2N91"
      },
      "outputs": [],
      "source": [
        "def imgrid(imarray, cols=4, pad=1, padval=255, row_major=True):\n",
        "  \"\"\"Lays out a [N, H, W, C] image array as a single image grid.\"\"\"\n",
        "  pad = int(pad)\n",
        "  if pad < 0:\n",
        "    raise ValueError('pad must be non-negative')\n",
        "  cols = int(cols)\n",
        "  assert cols >= 1\n",
        "  N, H, W, C = imarray.shape\n",
        "  rows = N // cols + int(N % cols != 0)\n",
        "  batch_pad = rows * cols - N\n",
        "  assert batch_pad >= 0\n",
        "  post_pad = [batch_pad, pad, pad, 0]\n",
        "  pad_arg = [[0, p] for p in post_pad]\n",
        "  imarray = np.pad(imarray, pad_arg, 'constant', constant_values=padval)\n",
        "  H += pad\n",
        "  W += pad\n",
        "  grid = (imarray\n",
        "          .reshape(rows, cols, H, W, C)\n",
        "          .transpose(0, 2, 1, 3, 4)\n",
        "          .reshape(rows*H, cols*W, C))\n",
        "  if pad:\n",
        "    grid = grid[:-pad, :-pad]\n",
        "  return grid\n",
        "\n",
        "def interleave(*args):\n",
        "  \"\"\"Interleaves input arrays of the same shape along the batch axis.\"\"\"\n",
        "  if not args:\n",
        "    raise ValueError('At least one argument is required.')\n",
        "  a0 = args[0]\n",
        "  if any(a.shape != a0.shape for a in args):\n",
        "    raise ValueError('All inputs must have the same shape.')\n",
        "  if not a0.shape:\n",
        "    raise ValueError('Inputs must have at least one axis.')\n",
        "  out = np.transpose(args, [1, 0] + list(range(2, len(a0.shape) + 1)))\n",
        "  out = out.reshape(-1, *a0.shape[1:])\n",
        "  return out\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  \"\"\"Displays an image in the given format.\"\"\"\n",
        "  a = a.astype(np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(a).save(data, format)\n",
        "  im_data = data.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print ('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format)\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def image_to_uint8(x):\n",
        "  \"\"\"Converts [-1, 1] float array to [0, 255] uint8.\"\"\"\n",
        "  x = np.asarray(x)\n",
        "  x = (256. / 2.) * (x + 1.)\n",
        "  x = np.clip(x, 0, 255)\n",
        "  x = x.astype(np.uint8)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ASXPMb6CaXR"
      },
      "source": [
        "## Load a BigBiGAN TF Hub module and display its available functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuG7G1ToCtaf"
      },
      "outputs": [],
      "source": [
        "# module = hub.Module(module_path, trainable=True, tags={'train'})  # training\n",
        "module = hub.Module(module_path)  # inference\n",
        "\n",
        "for signature in module.get_signature_names():\n",
        "  print('Signature:', signature)\n",
        "  print('Inputs:', pformat(module.get_input_info_dict(signature)))\n",
        "  print('Outputs:', pformat(module.get_output_info_dict(signature)))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAY-AmcNCj9_"
      },
      "source": [
        "## Define a wrapper class for convenient access to various functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTKHkxfx1dAL"
      },
      "outputs": [],
      "source": [
        "class BigBiGAN(object):\n",
        "\n",
        "  def __init__(self, module):\n",
        "    \"\"\"Initialize a BigBiGAN from the given TF Hub module.\"\"\"\n",
        "    self._module = module\n",
        "\n",
        "  def generate(self, z, upsample=False):\n",
        "    \"\"\"Run a batch of latents z through the generator to generate images.\n",
        "\n",
        "    Args:\n",
        "      z: A batch of 120D Gaussian latents, shape [N, 120].\n",
        "\n",
        "    Returns: a batch of generated RGB images, shape [N, 128, 128, 3], range\n",
        "      [-1, 1].\n",
        "    \"\"\"\n",
        "    outputs = self._module(z, signature='generate', as_dict=True)\n",
        "    return outputs['upsampled' if upsample else 'default']\n",
        "\n",
        "  def make_generator_ph(self):\n",
        "    \"\"\"Creates a tf.placeholder with the dtype & shape of generator inputs.\"\"\"\n",
        "    info = self._module.get_input_info_dict('generate')['z']\n",
        "    return tf.placeholder(dtype=info.dtype, shape=info.get_shape())\n",
        "\n",
        "  def gen_pairs_for_disc(self, z):\n",
        "    \"\"\"Compute generator input pairs (G(z), z) for discriminator, given z.\n",
        "\n",
        "    Args:\n",
        "      z: A batch of latents (120D standard Gaussians), shape [N, 120].\n",
        "\n",
        "    Returns: a tuple (G(z), z) of discriminator inputs.\n",
        "    \"\"\"\n",
        "    # Downsample 256x256 image x for 128x128 discriminator input.\n",
        "    x = self.generate(z)\n",
        "    return x, z\n",
        "\n",
        "  def encode(self, x, return_all_features=False):\n",
        "    \"\"\"Run a batch of images x through the encoder.\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "      return_all_features: If True, return all features computed by the encoder.\n",
        "        Otherwise (default) just return a sample z_hat.\n",
        "\n",
        "    Returns: the sample z_hat of shape [N, 120] (or a dict of all features if\n",
        "      return_all_features).\n",
        "    \"\"\"\n",
        "    outputs = self._module(x, signature='encode', as_dict=True)\n",
        "    return outputs if return_all_features else outputs['z_sample']\n",
        "\n",
        "  def make_encoder_ph(self):\n",
        "    \"\"\"Creates a tf.placeholder with the dtype & shape of encoder inputs.\"\"\"\n",
        "    info = self._module.get_input_info_dict('encode')['x']\n",
        "    return tf.placeholder(dtype=info.dtype, shape=info.get_shape())\n",
        "\n",
        "  def enc_pairs_for_disc(self, x):\n",
        "    \"\"\"Compute encoder input pairs (x, E(x)) for discriminator, given x.\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "\n",
        "    Returns: a tuple (downsample(x), E(x)) of discriminator inputs.\n",
        "    \"\"\"\n",
        "    # Downsample 256x256 image x for 128x128 discriminator input.\n",
        "    x_down = tf.nn.avg_pool(x, ksize=2, strides=2, padding='SAME')\n",
        "    z = self.encode(x)\n",
        "    return x_down, z\n",
        "\n",
        "  def discriminate(self, x, z):\n",
        "    \"\"\"Compute the discriminator scores for pairs of data (x, z).\n",
        "\n",
        "    (x, z) must be batches with the same leading batch dimension, and joint\n",
        "      scores are computed on corresponding pairs x[i] and z[i].\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (128x128 RGB images), shape [N, 128, 128, 3], range\n",
        "        [-1, 1].\n",
        "      z: A batch of latents (120D standard Gaussians), shape [N, 120].\n",
        "\n",
        "    Returns:\n",
        "      A dict of scores:\n",
        "        score_xz: the joint scores for the (x, z) pairs.\n",
        "        score_x: the unary scores for x only.\n",
        "        score_z: the unary scores for z only.\n",
        "    \"\"\"\n",
        "    inputs = dict(x=x, z=z)\n",
        "    return self._module(inputs, signature='discriminate', as_dict=True)\n",
        "\n",
        "  def reconstruct_x(self, x, use_sample=True, upsample=False):\n",
        "    \"\"\"Compute BigBiGAN reconstructions of images x via G(E(x)).\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "      use_sample: takes a sample z_hat ~ E(x). Otherwise, deterministically\n",
        "        use the mean. (Though a sample z_hat may be far from the mean z,\n",
        "        typically the resulting recons G(z_hat) and G(z) are very\n",
        "        similar.\n",
        "      upsample: if set, upsample the reconstruction to the input resolution\n",
        "        (256x256). Otherwise return the raw lower resolution generator output\n",
        "        (128x128).\n",
        "\n",
        "    Returns: a batch of recons G(E(x)), shape [N, 256, 256, 3] if\n",
        "      `upsample`, otherwise [N, 128, 128, 3].\n",
        "    \"\"\"\n",
        "    if use_sample:\n",
        "      z = self.encode(x)\n",
        "    else:\n",
        "      z = self.encode(x, return_all_features=True)['z_mean']\n",
        "    recons = self.generate(z, upsample=upsample)\n",
        "    return recons\n",
        "\n",
        "  def losses(self, x, z):\n",
        "    \"\"\"Compute per-module BigBiGAN losses given data & latent sample batches.\n",
        "\n",
        "    Args:\n",
        "      x: A batch of data (256x256 RGB images), shape [N, 256, 256, 3], range\n",
        "        [-1, 1].\n",
        "      z: A batch of latents (120D standard Gaussians), shape [M, 120].\n",
        "\n",
        "    For the original BigBiGAN losses, pass batches of size N=M=2048, with z's\n",
        "    sampled from a 120D standard Gaussian (e.g., np.random.randn(2048, 120)),\n",
        "    and x's sampled from the ImageNet (ILSVRC2012) training set with the\n",
        "    \"ResNet-style\" preprocessing from:\n",
        "\n",
        "        https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_preprocessing.py\n",
        "\n",
        "    Returns:\n",
        "      A dict of per-module losses:\n",
        "        disc: loss for the discriminator.\n",
        "        enc: loss for the encoder.\n",
        "        gen: loss for the generator.\n",
        "    \"\"\"\n",
        "    # Compute discriminator scores on (x, E(x)) pairs.\n",
        "    # Downsample 256x256 image x for 128x128 discriminator input.\n",
        "    scores_enc_x_dict = self.discriminate(*self.enc_pairs_for_disc(x))\n",
        "    scores_enc_x = tf.concat([scores_enc_x_dict['score_xz'],\n",
        "                              scores_enc_x_dict['score_x'],\n",
        "                              scores_enc_x_dict['score_z']], axis=0)\n",
        "\n",
        "    # Compute discriminator scores on (G(z), z) pairs.\n",
        "    scores_gen_z_dict = self.discriminate(*self.gen_pairs_for_disc(z))\n",
        "    scores_gen_z = tf.concat([scores_gen_z_dict['score_xz'],\n",
        "                              scores_gen_z_dict['score_x'],\n",
        "                              scores_gen_z_dict['score_z']], axis=0)\n",
        "\n",
        "    disc_loss_enc_x = tf.reduce_mean(tf.nn.relu(1. - scores_enc_x))\n",
        "    disc_loss_gen_z = tf.reduce_mean(tf.nn.relu(1. + scores_gen_z))\n",
        "    disc_loss = disc_loss_enc_x + disc_loss_gen_z\n",
        "\n",
        "    enc_loss = tf.reduce_mean(scores_enc_x)\n",
        "    gen_loss = tf.reduce_mean(-scores_gen_z)\n",
        "\n",
        "    return dict(disc=disc_loss, enc=enc_loss, gen=gen_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L5SFfH4C9gu"
      },
      "source": [
        "## Create tensors to be used later for computing samples, reconstructions, discriminator scores, and losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goxtzcb-19NA"
      },
      "outputs": [],
      "source": [
        "bigbigan = BigBiGAN(module)\n",
        "\n",
        "# Make input placeholders for x (`enc_ph`) and z (`gen_ph`).\n",
        "enc_ph = bigbigan.make_encoder_ph()\n",
        "gen_ph = bigbigan.make_generator_ph()\n",
        "\n",
        "# Compute samples G(z) from encoder input z (`gen_ph`).\n",
        "gen_samples = bigbigan.generate(gen_ph)\n",
        "\n",
        "# Compute reconstructions G(E(x)) of encoder input x (`enc_ph`).\n",
        "recon_x = bigbigan.reconstruct_x(enc_ph, upsample=True)\n",
        "\n",
        "# Compute encoder features used for representation learning evaluations given\n",
        "# encoder input x (`enc_ph`).\n",
        "enc_features = bigbigan.encode(enc_ph, return_all_features=True)\n",
        "\n",
        "# Compute discriminator scores for encoder pairs (x, E(x)) given x (`enc_ph`)\n",
        "# and generator pairs (G(z), z) given z (`gen_ph`).\n",
        "disc_scores_enc = bigbigan.discriminate(*bigbigan.enc_pairs_for_disc(enc_ph))\n",
        "disc_scores_gen = bigbigan.discriminate(*bigbigan.gen_pairs_for_disc(gen_ph))\n",
        "\n",
        "# Compute losses.\n",
        "losses = bigbigan.losses(enc_ph, gen_ph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly7LWnSUDQ_P"
      },
      "source": [
        "## Create a TensorFlow session and initialize variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPnzCHDWFJwx"
      },
      "outputs": [],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcEVS26D-ues"
      },
      "source": [
        "# Generator samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYSA8Zvb-w7S"
      },
      "source": [
        "First, we'll visualize samples from the pretrained BigBiGAN generator by sampling generator inputs `z` from a standard Gaussian (via `np.random.randn`) and displaying the images it produces. So far we're not going beyond the capabilites of a standard GAN -- we're just using the generator (and ignoring the encoder) for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zfpvw8fGNMr"
      },
      "outputs": [],
      "source": [
        "feed_dict = {gen_ph: np.random.randn(32, 120)}\n",
        "_out_samples = sess.run(gen_samples, feed_dict=feed_dict)\n",
        "print('samples shape:', _out_samples.shape)\n",
        "imshow(imgrid(image_to_uint8(_out_samples), cols=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v58CTfl8jTc"
      },
      "source": [
        "# Load `test_images` from the TF-Flowers dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0kmzQ4EqKJt"
      },
      "source": [
        "BigBiGAN is trained on ImageNet, but as it's too large to work with in this demo, we use the smaller TF-Flowers [1] dataset as our inputs for visualizing reconstructions and computing encoder features.\n",
        "\n",
        "In this cell we load TF-Flowers (downloading the dataset if needed) and store a fixed batch of 256x256 RGB image samples in a NumPy array `test_images`.\n",
        "\n",
        "[1] https://www.tensorflow.org/datasets/catalog/tf_flowers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBgpkMdkUjL-"
      },
      "outputs": [],
      "source": [
        "def get_flowers_data():\n",
        "  \"\"\"Returns a [32, 256, 256, 3] np.array of preprocessed TF-Flowers samples.\"\"\"\n",
        "  import tensorflow_datasets as tfds\n",
        "  ds, info = tfds.load('tf_flowers', split='train', with_info=True)\n",
        "\n",
        "  # Just get the images themselves as we don't need labels for this demo.\n",
        "  ds = ds.map(lambda x: x['image'])\n",
        "\n",
        "  # Filter out small images (with minor edge length <256).\n",
        "  ds = ds.filter(lambda x: tf.reduce_min(tf.shape(x)[:2]) >= 256)\n",
        "\n",
        "  # Take the center square crop of the image and resize to 256x256.\n",
        "  def crop_and_resize(image):\n",
        "    imsize = tf.shape(image)[:2]\n",
        "    minor_edge = tf.reduce_min(imsize)\n",
        "    start = (imsize - minor_edge) // 2\n",
        "    stop = start + minor_edge\n",
        "    cropped_image = image[start[0] : stop[0], start[1] : stop[1]]\n",
        "    resized_image = tf.image.resize_bicubic([cropped_image], [256, 256])[0]\n",
        "    return resized_image\n",
        "  ds = ds.map(crop_and_resize)\n",
        "\n",
        "  # Convert images from [0, 255] uint8 to [-1, 1] float32.\n",
        "  ds = ds.map(lambda image: tf.cast(image, tf.float32) / (255. / 2.) - 1)\n",
        "\n",
        "  # Take the first 32 samples.\n",
        "  ds = ds.take(32)\n",
        "\n",
        "  return np.array(list(tfds.as_numpy(ds)))\n",
        "\n",
        "test_images = get_flowers_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAFJQU597n2A"
      },
      "source": [
        "# Reconstructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmCQ9N9b7ptM"
      },
      "source": [
        "Now we visualize BigBiGAN reconstructions by passing real images through the encoder and back through the generator, computing `G(E(x))` given images `x`.\n",
        "Below, input images `x` are shown in the left column, and corresponding reconstructions are shown on the right.\n",
        "\n",
        "Note that reconstructions are not pixel-perfect matches to the input images; rather, they tend to capture the higher level semantic content of the input while \"forgetting\" most of the low-level detail. This suggests the BigBiGAN encoder may learn to capture the types of high level semantic information about images that we'd like to see in a representation learning approach.\n",
        "\n",
        "Also note that the raw reconstructions of the 256x256 input images are at the lower resolution produced by our generator -- 128x128. We upsample them for visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2F3eq8aFRle"
      },
      "outputs": [],
      "source": [
        "test_images_batch = test_images[:16]\n",
        "_out_recons = sess.run(recon_x, feed_dict={enc_ph: test_images_batch})\n",
        "print('reconstructions shape:', _out_recons.shape)\n",
        "\n",
        "inputs_and_recons = interleave(test_images_batch, _out_recons)\n",
        "print('inputs_and_recons shape:', inputs_and_recons.shape)\n",
        "imshow(imgrid(image_to_uint8(inputs_and_recons), cols=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPpW3qdbEpXL"
      },
      "source": [
        "# Encoder features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gAW76YxEsZa"
      },
      "source": [
        "We now demonstrate how to compute features from the encoder used for standard representation learning evaluations.\n",
        "\n",
        "These features could be used in a linear or nearest neighbors-based classifier. We include the standard feature taken after the global average pooling (key `avepool_feat`) as well as the larger \"BN+CReLU\" feature (key `bn_crelu_feat`) used to achieve the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpZYe5S_FQEw"
      },
      "outputs": [],
      "source": [
        "_out_features = sess.run(enc_features, feed_dict={enc_ph: test_images_batch})\n",
        "print('AvePool features shape:', _out_features['avepool_feat'].shape)\n",
        "print('BN+CReLU features shape:', _out_features['bn_crelu_feat'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGzahsms2w9a"
      },
      "source": [
        "# Discriminator scores and losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_5BIBN21Hr"
      },
      "source": [
        "Finally, we'll compute the discriminator scores and losses on batches of encoder and generator pairs. These losses could be passed into an optimizer to train BigBiGAN.\n",
        "\n",
        "We use our batch of images above as the encoder inputs `x`, computing the encoder score as `D(x, E(x))`. For the generator inputs we sample `z` from a 120D standard Gaussian via `np.random.randn`, computing the generator score as `D(G(z), z)`.\n",
        "\n",
        "The discriminator predicts a joint score `score_xz` for the `(x, z)` pairs as well as unary scores `score_x` and `score_z` for `x` and `z` alone, respectively. It's trained to give high (positive) scores to encoder pairs and low (negative) scores to generator pairs. This mostly holds below, though the unary `score_z` is negative in both cases, indicating that the encoder outputs `E(x)` resemble actual samples from a Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JJ8Go0dr22-"
      },
      "outputs": [],
      "source": [
        "feed_dict = {enc_ph: test_images, gen_ph: np.random.randn(32, 120)}\n",
        "_out_scores_enc, _out_scores_gen, _out_losses = sess.run(\n",
        "    [disc_scores_enc, disc_scores_gen, losses], feed_dict=feed_dict)\n",
        "print('Encoder scores:', {k: v.mean() for k, v in _out_scores_enc.items()})\n",
        "print('Generator scores:', {k: v.mean() for k, v in _out_scores_gen.items()})\n",
        "print('Losses:', _out_losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9v58CTfl8jTc"
      ],
      "name": "bigbigan_with_tf_hub.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
