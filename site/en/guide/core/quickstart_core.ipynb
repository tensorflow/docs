{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "# Quickstart for the TensorFlow Core APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUNzJc4jTj6G"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/core/quickstart_core\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/core/quickstart_core.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/core/quickstart_core.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/core/quickstart_core.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04QgGZc9bF5D"
      },
      "source": [
        "This short introduction uses the [TensorFlow Core low-level APIs](https://www.tensorflow.org/guide/core) to walk through the following stages of a machine learning example:\n",
        "\n",
        "1. Load a prebuilt dataset.\n",
        "1. Build a multiple linear regression model that predicts fuel efficiency.\n",
        "2. Train this multiple linear regression model.\n",
        "3. Evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiH7AC-NTniF"
      },
      "source": [
        "This tutorial is a [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb) notebook. Python programs are run directly in the browserâ€”a great way to learn and use TensorFlow. To follow this tutorial, run the notebook in Google Colab by clicking the button at the top of this page.\n",
        "\n",
        "1. In Colab, connect to a Python runtime: At the top-right of the menu bar, select *CONNECT*.\n",
        "2. Run all the notebook code cells: Select *Runtime* > *Run all*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnrWf3PCEzXL"
      },
      "source": [
        "## Set up TensorFlow\n",
        "\n",
        "Import TensorFlow and pandas into your program to get started:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0trJmd6DjqBZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "# Set a random seed for reproducible results \n",
        "tf.random.set_seed(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NAbSZiaoJ4z"
      },
      "source": [
        "If you are following along in your own development environment, rather than [Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/core/quickstart_core.ipynb), see the [install guide](https://www.tensorflow.org/install) for setting up TensorFlow for development.\n",
        "\n",
        "Note: Make sure you have upgraded to the latest `pip` to install the TensorFlow 2 package if you are using your own development environment. See the [install guide](https://www.tensorflow.org/install) for details.\n",
        "\n",
        "## Load a dataset\n",
        "\n",
        "Load and prepare the [Auto MPG dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg) from the UCI Machine Learning Repository. This dataset uses a variety of quantitative and categorical features to predict the fuel efficiencies of automobiles in the late-1970s and early 1980s. Make sure to drop any missing values in the dataset and convert it to a tf.float32 tensor with the `tf.convert_to_tensor` and `tf.cast` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HglhDsUfrJ98"
      },
      "outputs": [],
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "dataset = pd.read_csv(url, names=column_names, na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "\n",
        "dataset = dataset.dropna()\n",
        "dataset_tf = tf.convert_to_tensor(dataset, dtype=tf.float32)\n",
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vgoDL3hYesB"
      },
      "source": [
        "Next split the dataset into [training and testing groups](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). Make sure to shuffle the dataset with `tf.random.shuffle` to avoid biased splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mJU4kt6YiAp"
      },
      "outputs": [],
      "source": [
        "dataset_shuffled = tf.random.shuffle(dataset_tf, seed=22)\n",
        "train_data, test_data = dataset_shuffled[100:], dataset_shuffled[:100]\n",
        "x_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
        "x_test, y_test = test_data[:, 1:], test_data[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bscb2Vsbi3TE"
      },
      "source": [
        "Perform basic [feature engineering](https://developers.google.com/machine-learning/crash-course/representation/feature-engineering) by one-hot-encoding the `\"Origin\"` feature. The `tf.one_hot` function is useful for transforming this categorical column into 3 separate binary columns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B8N9IV1i6IV"
      },
      "outputs": [],
      "source": [
        "def onehot_origin(x):\n",
        "  origin = tf.cast(x[:, -1], tf.int32)\n",
        "  # Use origin - 1 to account for 1-indexed feature\n",
        "  origin_oh = tf.one_hot(origin - 1, 3)\n",
        "  x_ohe = tf.concat([x[:, :-1], origin_oh], axis = 1)\n",
        "  return x_ohe\n",
        "\n",
        "x_train_ohe, x_test_ohe = onehot_origin(x_train), onehot_origin(x_test)\n",
        "x_train_ohe.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnoCDzzedite"
      },
      "source": [
        "This example shows a multiple regression problem with predictors or features on vastly different scales. Therefore, it is beneficial to standardize the data so that each feature has zero mean and unit variance. Use the `tf.reduce_mean` and `tf.math.reduce_std` functions for standardization. The regression model's prediction can then be unstandardized to obtain its value in terms of the original units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJJFdvqydhyp"
      },
      "outputs": [],
      "source": [
        "class Normalize(tf.Module):\n",
        "  def __init__(self, x):\n",
        "    # Initialize the mean and standard deviation for normalization\n",
        "    self.mean = tf.math.reduce_mean(x, axis=0)\n",
        "    self.std = tf.math.reduce_std(x, axis=0)\n",
        "\n",
        "  def norm(self, x):\n",
        "    # Normalize the input\n",
        "    return (x - self.mean)/self.std\n",
        "\n",
        "  def unnorm(self, x):\n",
        "    # Unnormalize the input\n",
        "    return (x * self.std) + self.mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BONV6fYYwZb"
      },
      "outputs": [],
      "source": [
        "norm_x = Normalize(x_train_ohe)\n",
        "norm_y = Normalize(y_train)\n",
        "x_train_norm, y_train_norm = norm_x.norm(x_train_ohe), norm_y.norm(y_train)\n",
        "x_test_norm, y_test_norm = norm_x.norm(x_test_ohe), norm_y.norm(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "## Build a machine learning model\n",
        "\n",
        "Build a linear regression model with the TensorFlow Core APIs. The equation for multiple linear regression is as follows:\n",
        "\n",
        "$${\\mathrm{Y}} = {\\mathrm{X}}w + b$$\n",
        "\n",
        "where\n",
        "\n",
        "* $\\underset{m\\times 1}{\\mathrm{Y}}$: target vector\n",
        "* $\\underset{m\\times n}{\\mathrm{X}}$: feature matrix\n",
        "* $\\underset{n\\times 1}w$: weight vector\n",
        "* $b$: bias\n",
        "\n",
        "By using the `@tf.function` decorator, the corresponding Python code is traced to generate a callable TensorFlow graph. This approach is beneficial for saving and loading the model after training. It can also provide a performance boost for models with many layers and complex operations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.built = False\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    # Initialize the model parameters on the first call\n",
        "    if not self.built:\n",
        "      # Randomly generate the weight vector and bias term\n",
        "      rand_w = tf.random.uniform(shape=[x.shape[-1], 1])\n",
        "      rand_b = tf.random.uniform(shape=[])\n",
        "      self.w = tf.Variable(rand_w)\n",
        "      self.b = tf.Variable(rand_b)\n",
        "      self.built = True\n",
        "    y = tf.add(tf.matmul(x, self.w), self.b)\n",
        "    return tf.squeeze(y, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2hiez2eIUz8"
      },
      "source": [
        "For each example, the model returns a prediction for the input automobile's MPG by computing the weighted sum of its features plus a bias term. This prediction can then be unstandardized to obtain its value in terms of the original units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeOrNdnkEEcR"
      },
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "prediction = lin_reg(x_train_norm[:1])\n",
        "prediction_unnorm = norm_y.unnorm(prediction)\n",
        "prediction_unnorm.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIHANxNSvWr9"
      },
      "source": [
        "## Defining a loss function\n",
        "\n",
        "Now define a loss function to evaluate the model's performance during the training process.\n",
        "\n",
        "Since regression problems deal with continuous outputs, the mean squared error (MSE) is an ideal choice for the loss function. The MSE is defined by the following equation:\n",
        "\n",
        "$$MSE = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_i -y_i)^2$$\n",
        "\n",
        "where\n",
        "\n",
        "* $\\hat{y}$: vector of predictions\n",
        "* $y$: vector of true targets\n",
        "\n",
        "The goal of this regression problem is to find the optimal weight vector, $w$, and bias, $b$, that minimizes the MSE loss function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tYNVUkmw35s"
      },
      "outputs": [],
      "source": [
        "def mse_loss(y_pred, y):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix4mEL65on-w"
      },
      "source": [
        "## Train and evaluate your model\n",
        "\n",
        "Write a training loop to iteratively update your model's parameters by making use of the MSE loss function and its gradients with respect to the input parameters.\n",
        "\n",
        "This iterative method is referred to as gradient descent. At each iteration, the model's parameters are updated by taking a step in the opposite direction of their computed gradients. The size of this step is determined by the learning rate, which is a configurable hyperparameter. Recall that the gradient of a function indicates the direction of its steepest ascent; therefore, taking a step in the opposite direction indicates the direction of steepest descent, which ultimately helps to minimize the MSE loss function.\n",
        "\n",
        "Using mini-batches for training provides both memory efficiency and faster convergence. The `tf.data.Dataset` API has useful functions for batching and shuffling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS9hB17NY3sf"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train_norm))\n",
        "dataset = dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7suUbJXVLqP"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "# Format training loop\n",
        "for epoch in range(epochs):\n",
        "  for x_batch, y_batch in dataset:\n",
        "    batch_losses = []\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred_batch = lin_reg(x_batch)\n",
        "      batch_loss = mse_loss(y_pred_batch, y_batch)\n",
        "    # Update parameters with respect to the gradient calculations\n",
        "    grads = tape.gradient(batch_loss, lin_reg.variables)\n",
        "    for g,v in zip(grads, lin_reg.variables):\n",
        "      v.assign_sub(learning_rate * g)\n",
        "    # Keep track of batch-level model performance \n",
        "    batch_losses.append(batch_loss)\n",
        "  # Keep track of epoch-level model performance\n",
        "  train_loss = tf.reduce_mean(batch_losses)\n",
        "  test_loss = mse_loss(lin_reg(x_test_norm), y_test_norm)\n",
        "  train_losses.append(train_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  if epoch % 10 == 0:\n",
        "    print(f'Mean squared error for step {epoch}: {train_loss.numpy():0.3f}')\n",
        "\n",
        "# Output final losses\n",
        "print(f\"\\nFinal train loss: {train_loss:0.3f}\")\n",
        "print(f\"Final test loss: {test_loss:0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mDAAPFqVVgn"
      },
      "source": [
        "Plot the changes in MSE loss over time. Calculating performance metrics on a designated [Validation-set](https://developers.google.com/machine-learning/glossary#validation-set) or [Test-set](https://developers.google.com/machine-learning/glossary#test-set) ensures the model does not overfit to the training dataset and can generalize well to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7dTAzgHDUh7"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
        "\n",
        "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
        "plt.plot(range(epochs), test_losses, label = \"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean squared error loss\")\n",
        "plt.legend()\n",
        "plt.title(\"MSE loss vs training iterations\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj8NrlzlJqDG"
      },
      "source": [
        "It seems like the model does a good job of fitting the training data while also generalizing well to the unseen test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUNIPubuPYDR"
      },
      "source": [
        "## Saving your model\n",
        "\n",
        "Start by making an export module that takes in raw data and performs the following operations:\n",
        "- Feature extraction \n",
        "- Normalization \n",
        "- Prediction\n",
        "- Unnormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-uOrGa9ZehG"
      },
      "outputs": [],
      "source": [
        "class ExportModule(tf.Module):\n",
        "  def __init__(self, model, extract_features, norm_x, norm_y):\n",
        "    # Initialize pre and postprocessing functions\n",
        "    self.model = model\n",
        "    self.extract_features = extract_features\n",
        "    self.norm_x = norm_x\n",
        "    self.norm_y = norm_y\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None], dtype=tf.float32)]) \n",
        "  def __call__(self, x):\n",
        "    # Run the ExportModule for new data points\n",
        "    x = self.extract_features(x)\n",
        "    x = self.norm_x.norm(x)\n",
        "    y = self.model(x)\n",
        "    y = self.norm_y.unnorm(y)\n",
        "    return y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPYYLQ8EZiU8"
      },
      "outputs": [],
      "source": [
        "lin_reg_export = ExportModule(model=lin_reg,\n",
        "                              extract_features=onehot_origin,\n",
        "                              norm_x=norm_x,\n",
        "                              norm_y=norm_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v8xi06XZWiC"
      },
      "source": [
        "If you want to save the model at its current state, you can do so with the `tf.saved_model.save` function. To load a saved model and make predictions, use the `tf.saved_model.load` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1IvMoHbptht"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "models = tempfile.mkdtemp()\n",
        "save_path = os.path.join(models, 'lin_reg_export')\n",
        "tf.saved_model.save(lin_reg_export, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "outputs": [],
      "source": [
        "lin_reg_loaded = tf.saved_model.load(save_path)\n",
        "test_preds = lin_reg_loaded(x_test)\n",
        "test_preds[:10].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-47O6_GLdRuT"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You have trained a machine learning model with a prebuilt dataset using the [TensorFlow Core APIs](https://www.tensorflow.org/guide/core). If you want learn more about loading and preparing data, see the tutorials on [image data loading](https://www.tensorflow.org/tutorials/load_data/images) or [CSV data loading](https://www.tensorflow.org/tutorials/load_data/csv).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rX8mhOLljYeM"
      ],
      "name": "quickstart_core.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
