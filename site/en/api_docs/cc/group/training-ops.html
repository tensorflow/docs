<!DOCTYPE html>
<html devsite>
<head>
  <title>Training Ops</title>
  <meta name="project_path" value="/_project.yaml" />
  <meta name="book_path" value="/versions/r1.1/_book.yaml" />
  <meta name="hide_page_heading" value="true" />
</head>
<body>
  <div id="top"><!-- do not remove this div --></div>
  <h1>Training Ops</h1>
  <h2>Summary</h2>
  <table class="nested-classes responsive">
    <tr>
      <th colspan="2">
        <h3>Classes</h3>
      </th>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-adadelta">tensorflow::<wbr/>ops::<wbr/>ApplyAdadelta</a>
      </td>
      <td>
        <p>Update '*var' according to the adadelta scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-adagrad">tensorflow::<wbr/>ops::<wbr/>ApplyAdagrad</a>
      </td>
      <td>
        <p>Update '*var' according to the adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-adagrad-d-a">tensorflow::<wbr/>ops::<wbr/>ApplyAdagradDA</a>
      </td>
      <td>
        <p>Update '*var' according to the proximal adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-adam">tensorflow::<wbr/>ops::<wbr/>ApplyAdam</a>
      </td>
      <td>
        <p>Update '*var' according to the Adam algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-centered-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>ApplyCenteredRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the centered RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-ftrl">tensorflow::<wbr/>ops::<wbr/>ApplyFtrl</a>
      </td>
      <td>
        <p>Update '*var' according to the Ftrl-proximal scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-gradient-descent">tensorflow::<wbr/>ops::<wbr/>ApplyGradientDescent</a>
      </td>
      <td>
        <p>Update '*var' by subtracting 'alpha' * 'delta' from it. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-momentum">tensorflow::<wbr/>ops::<wbr/>ApplyMomentum</a>
      </td>
      <td>
        <p>Update '*var' according to the momentum scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-proximal-adagrad">tensorflow::<wbr/>ops::<wbr/>ApplyProximalAdagrad</a>
      </td>
      <td>
        <p>Update '*var' and '*accum' according to FOBOS with Adagrad learning rate. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-proximal-gradient-descent">tensorflow::<wbr/>ops::<wbr/>ApplyProximalGradientDescent</a>
      </td>
      <td>
        <p>Update '*var' as FOBOS algorithm with fixed learning rate. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/apply-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>ApplyRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-adadelta">tensorflow::<wbr/>ops::<wbr/>ResourceApplyAdadelta</a>
      </td>
      <td>
        <p>Update '*var' according to the adadelta scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-adagrad">tensorflow::<wbr/>ops::<wbr/>ResourceApplyAdagrad</a>
      </td>
      <td>
        <p>Update '*var' according to the adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-adagrad-d-a">tensorflow::<wbr/>ops::<wbr/>ResourceApplyAdagradDA</a>
      </td>
      <td>
        <p>Update '*var' according to the proximal adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-adam">tensorflow::<wbr/>ops::<wbr/>ResourceApplyAdam</a>
      </td>
      <td>
        <p>Update '*var' according to the Adam algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-centered-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>ResourceApplyCenteredRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the centered RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-ftrl">tensorflow::<wbr/>ops::<wbr/>ResourceApplyFtrl</a>
      </td>
      <td>
        <p>Update '*var' according to the Ftrl-proximal scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-gradient-descent">tensorflow::<wbr/>ops::<wbr/>ResourceApplyGradientDescent</a>
      </td>
      <td>
        <p>Update '*var' by subtracting 'alpha' * 'delta' from it. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-momentum">tensorflow::<wbr/>ops::<wbr/>ResourceApplyMomentum</a>
      </td>
      <td>
        <p>Update '*var' according to the momentum scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-proximal-adagrad">tensorflow::<wbr/>ops::<wbr/>ResourceApplyProximalAdagrad</a>
      </td>
      <td>
        <p>Update '*var' and '*accum' according to FOBOS with Adagrad learning rate. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-proximal-gradient-descent">tensorflow::<wbr/>ops::<wbr/>ResourceApplyProximalGradientDescent</a>
      </td>
      <td>
        <p>Update '*var' as FOBOS algorithm with fixed learning rate. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-apply-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>ResourceApplyRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-adadelta">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyAdadelta</a>
      </td>
      <td>
        <p>var: Should be from a Variable(). </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-adagrad">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyAdagrad</a>
      </td>
      <td>
        <p>Update relevant entries in '*var' and '*accum' according to the adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-adagrad-d-a">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyAdagradDA</a>
      </td>
      <td>
        <p>Update entries in '*var' and '*accum' according to the proximal adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-centered-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyCenteredRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the centered RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-ftrl">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyFtrl</a>
      </td>
      <td>
        <p>Update relevant entries in '*var' according to the Ftrl-proximal scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-momentum">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyMomentum</a>
      </td>
      <td>
        <p>Update relevant entries in '*var' and '*accum' according to the momentum scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-proximal-adagrad">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyProximalAdagrad</a>
      </td>
      <td>
        <p>Sparse update entries in '*var' and '*accum' according to FOBOS algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-proximal-gradient-descent">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyProximalGradientDescent</a>
      </td>
      <td>
        <p>Sparse update '*var' as FOBOS algorithm with fixed learning rate. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/resource-sparse-apply-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>ResourceSparseApplyRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-adadelta">tensorflow::<wbr/>ops::<wbr/>SparseApplyAdadelta</a>
      </td>
      <td>
        <p>var: Should be from a Variable(). </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-adagrad">tensorflow::<wbr/>ops::<wbr/>SparseApplyAdagrad</a>
      </td>
      <td>
        <p>Update relevant entries in '*var' and '*accum' according to the adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-adagrad-d-a">tensorflow::<wbr/>ops::<wbr/>SparseApplyAdagradDA</a>
      </td>
      <td>
        <p>Update entries in '*var' and '*accum' according to the proximal adagrad scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-centered-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>SparseApplyCenteredRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the centered RMSProp algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-ftrl">tensorflow::<wbr/>ops::<wbr/>SparseApplyFtrl</a>
      </td>
      <td>
        <p>Update relevant entries in '*var' according to the Ftrl-proximal scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-momentum">tensorflow::<wbr/>ops::<wbr/>SparseApplyMomentum</a>
      </td>
      <td>
        <p>Update relevant entries in '*var' and '*accum' according to the momentum scheme. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-proximal-adagrad">tensorflow::<wbr/>ops::<wbr/>SparseApplyProximalAdagrad</a>
      </td>
      <td>
        <p>Sparse update entries in '*var' and '*accum' according to FOBOS algorithm. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-proximal-gradient-descent">tensorflow::<wbr/>ops::<wbr/>SparseApplyProximalGradientDescent</a>
      </td>
      <td>
        <p>Sparse update '*var' as FOBOS algorithm with fixed learning rate. </p>
      </td>
    </tr>
    <tr>
      <td>
        <a href="/versions/r1.1/api_docs/cc/class/tensorflow/ops/sparse-apply-r-m-s-prop">tensorflow::<wbr/>ops::<wbr/>SparseApplyRMSProp</a>
      </td>
      <td>
        <p>Update '*var' according to the RMSProp algorithm. </p>
      </td>
    </tr>
  </table>
</body>
</html>
