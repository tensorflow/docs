{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6bYaCABobL5q"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "FlUw7tSKbtg4"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xc1srSc51n_4"
      },
      "source": [
        "# Using the SavedModel format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-nBUqG2rchGH"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/alpha/guide/saved_model\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/saved_model.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/saved_model.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CPE-fshLTsXU"
      },
      "source": [
        "A SavedModel contains a complete TensorFlow program, including weights and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying (with [TFLite](https://tensorflow.org/lite), [TensorFlow.js](https://js.tensorflow.org/), [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple), or [TFHub](https://tensorflow.org/hub)).\n",
        "\n",
        "If you have code for a model in Python and want to load weights into it, see the [guide to training checkpoints](./checkpoints.ipynb).\n",
        "\n",
        "\n",
        "\n",
        "For a quick introduction, this section exports a pre-trained Keras model and serves image classification requests with it. The rest of the guide will fill in details and discuss other ways to create SavedModels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Le5OB-fBHHW7"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SofdPKo0G8Lb"
      },
      "outputs": [],
      "source": [
        "file = tf.keras.utils.get_file(\n",
        "    \"grace_hopper.jpg\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\")\n",
        "img = tf.keras.preprocessing.image.load_img(file, target_size=[224, 224])\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "x = tf.keras.applications.mobilenet.preprocess_input(\n",
        "    np.array(img)[tf.newaxis,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sqVcFL10JkF0"
      },
      "source": [
        "We'll use an image of Grace Hopper as a running example, and a Keras pre-trained image classification model since it's easy to use. Custom models work too, and are covered in detail later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JhVecdzJTsKE"
      },
      "outputs": [],
      "source": [
        "#tf.keras.applications.vgg19.decode_predictions\n",
        "labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
        "imagenet_labels = np.array(open(labels_path).read().splitlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aEHSYjW6JZHV"
      },
      "outputs": [],
      "source": [
        "pretrained_model = tf.keras.applications.MobileNet()\n",
        "result_before_save = pretrained_model(x)\n",
        "print()\n",
        "\n",
        "decoded = imagenet_labels[np.argsort(result_before_save)[0,::-1][:5]+1]\n",
        "\n",
        "print(\"Result before saving:\\n\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r4KIsQDZJ5PS"
      },
      "source": [
        "The top prediction for this image is \"military uniform\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8nfznDmHCW6F"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(pretrained_model, \"/tmp/mobilenet/1/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pyX-ETE3wX63"
      },
      "source": [
        "The save-path follows a convention used by TensorFlow Serving where the last path component (`1/` here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.\n",
        "\n",
        "SavedModels have named functions called signatures. Keras models export their forward pass under the `serving_default` signature key. The [SavedModel command line interface](#saved_model_cli) is useful for inspecting SavedModels on disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "djmcTavtIZyT"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VCZZ8avqLF1g"
      },
      "source": [
        "We can load the SavedModel back into Python with `tf.saved_model.load` and see how Admiral Hopper's image is classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NP2UpVFRV7N_"
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(\"/tmp/mobilenet/1/\")\n",
        "print(list(loaded.signatures.keys()))  # [\"serving_default\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K5srGzowfWff"
      },
      "source": [
        "Imported signatures always return dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ChFLpegYfQGR"
      },
      "outputs": [],
      "source": [
        "infer = loaded.signatures[\"serving_default\"]\n",
        "print(infer.structured_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cJYyZnptfuru"
      },
      "source": [
        "Running inference from the SavedModel gives the same result as the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9WjGEaS3XfX7"
      },
      "outputs": [],
      "source": [
        "labeling = infer(tf.constant(x))[\"reshape_2\"]\n",
        "\n",
        "decoded = imagenet_labels[np.argsort(labeling)[0,::-1][:5]+1]\n",
        "\n",
        "print(\"Result after saving and loading:\\n\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SJEkdXjTWbtl"
      },
      "source": [
        "## Serving the model\n",
        "\n",
        "SavedModels are usable from Python, but production environments will typically want a dedicated service for inference. This is easy to set up from a SavedModel using TensorFlow Serving.\n",
        "\n",
        "See the [TensorFlow Serving REST tutorial](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/tutorials/Serving_REST_simple.ipynb) for more details about serving, including instructions for installing `tensorflow_model_server` in a notebook or on your local machine. As a quick sketch, to serve the `mobilenet` model exported above just point the model server at the SavedModel directory:\n",
        "\n",
        "```bash\n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=mobilenet \\\n",
        "  --model_base_path=\"/tmp/mobilenet\" \u003eserver.log 2\u003e\u00261\n",
        "```\n",
        "\n",
        "  Then send a request.\n",
        "\n",
        "```python\n",
        "!pip install -q requests\n",
        "import json\n",
        "import numpy\n",
        "import requests\n",
        "data = json.dumps({\"signature_name\": \"serving_default\",\n",
        "                   \"instances\": x.tolist()})\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/mobilenet:predict',\n",
        "                              data=data, headers=headers)\n",
        "predictions = numpy.array(json.loads(json_response.text)[\"predictions\"])\n",
        "```\n",
        "\n",
        "The resulting `predictions` are identical to the results from Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bi0ILzu1XdWw"
      },
      "source": [
        "# SavedModel format\n",
        "\n",
        "A SavedModel is a directory containing serialized signatures and the state needed to run them like variable values and vocabularies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6u3YZuYZXyTO"
      },
      "outputs": [],
      "source": [
        "!ls /tmp/mobilenet/1  # assets\tsaved_model.pb\tvariables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ple4X5utX8ue"
      },
      "source": [
        "The `saved_model.pb` file contains a set of named signatures, each identifying a function.\n",
        "\n",
        "SavedModels may contain multiple sets of signatures (multiple MetaGraphs, identified with the `tag_set` argument to `saved_model_cli`), but this is rare. APIs which create multiple sets of signatures include [`tf.Estimator.experimental_export_all_saved_models`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#experimental_export_all_saved_models) and in TensorFlow 1.x `tf.saved_model.Builder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Pus0dOYTYXbI"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eALHpGvRZOhk"
      },
      "source": [
        "The `variables` directory contains a standard training checkpoint (see the [guide to training checkpoints](./checkpoints.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EDYqhDlNZAC2"
      },
      "outputs": [],
      "source": [
        "!ls /tmp/mobilenet/1/variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VKmaZQpHahGh"
      },
      "source": [
        "The `assets` directory contains files used by the TensorFlow graph, for example text files used to initialize vocabulary tables. It is unused in this example.\n",
        "\n",
        "SavedModels may have an `assets.extra` directory for any files not used by the TensorFlow graph, for example information for consumers about what to do with the SavedModel. TensorFlow itself does not use this directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIceoF_CYmaF"
      },
      "source": [
        "# Exporting custom models\n",
        "\n",
        "In the first section, `tf.saved_model.save` automatically determined a signature for the `tf.keras.Model` object. This worked because Keras `Model` objects have an unambiguous method to export and known input shapes. `tf.saved_model.save` works just as well with low-level model building APIs, but you will need to indicate which function to use as a signature if you're planning to serve a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6EPvKiqXMm3d"
      },
      "outputs": [],
      "source": [
        "class CustomModule(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CustomModule, self).__init__()\n",
        "    self.v = tf.Variable(1.)\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x):\n",
        "    return x * self.v\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec([], tf.float32)])\n",
        "  def mutate(self, new_v):\n",
        "    self.v.assign(new_v)\n",
        "\n",
        "module = CustomModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fUrCTSK2HV2b"
      },
      "source": [
        "This module has two methods decorated with `tf.function`. While these functions will be included in the SavedModel and available if the SavedModel is reloaded via `tf.saved_model.load` into a Python program, without explicitly declaring the serving signature tools like Tensorflow Serving and `saved_model_cli` cannot access them.\n",
        "\n",
        "`module.mutate` has an `input_signature`, and so there is enough information to save its computation graph in the SavedModel already. `__call__` has no signature and so this method needs to be called before saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "85PUO9iWH7xn"
      },
      "outputs": [],
      "source": [
        "module(tf.constant(0.))\n",
        "tf.saved_model.save(module, \"/tmp/module_no_signatures\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eyWD4wr-Ng7m"
      },
      "source": [
        "For functions without an `input_signature`, any input shapes used before saving will be available after loading. Since we called `__call__` with just a scalar, it will accept only scalar values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xy7oCex1Ibj1"
      },
      "outputs": [],
      "source": [
        "imported = tf.saved_model.load(\"/tmp/module_no_signatures\")\n",
        "assert 3. == imported(tf.constant(3.)).numpy()\n",
        "imported.mutate(tf.constant(2.))\n",
        "assert 6. == imported(tf.constant(3.)).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lbLNVfVJOTfb"
      },
      "source": [
        "The function will not accept new shapes like vectors.\n",
        "\n",
        "```python\n",
        "imported(tf.constant([3.]))\n",
        "```\n",
        "\n",
        "\u003cpre\u003e\n",
        "ValueError: Could not find matching function to call for canonicalized inputs ((\u003ctf.Tensor 'args_0:0' shape=(1,) dtype=float32\u003e,), {}). Only existing signatures are [((TensorSpec(shape=(), dtype=tf.float32, name=u'x'),), {})].\n",
        "\u003c/pre\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WrJqD-epPGnr"
      },
      "source": [
        "`get_concrete_function` lets you add input shapes to a function without calling it. It takes `tf.TensorSpec` objects in place of `Tensor` arguments, indicating the shapes and dtypes of inputs. Shapes can either be `None`, indicating that any shape is acceptable, or a list of axis sizes. If an axis size is `None` then any size is acceptable for that axis. `tf.TensorSpecs` can also have names, which default to the function's argument keywords (\"x\" here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1m9Okb75PFmb"
      },
      "outputs": [],
      "source": [
        "module.__call__.get_concrete_function(x=tf.TensorSpec([None], tf.float32))\n",
        "tf.saved_model.save(module, \"/tmp/module_no_signatures\")\n",
        "imported = tf.saved_model.load(\"/tmp/module_no_signatures\")\n",
        "assert [3.] == imported(tf.constant([3.])).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gvy3GFl4IfSW"
      },
      "source": [
        "Functions and variables attached to objects like `tf.keras.Model` and `tf.Module` are available on import, but many Python types and attributes are lost. The Python program itself is not saved in the SavedModel.\n",
        "\n",
        "We didn't identify any of the functions we exported as a signature, so it has none."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uNTV6o_TIeRu"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir /tmp/module_no_signatures --tag_set serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BiNtaMZSI8Tb"
      },
      "source": [
        "## Identifying a signature to export\n",
        "\n",
        "To indicate that a function should be a signature, specify the `signatures` argument when saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_pAdgIORR2yH"
      },
      "outputs": [],
      "source": [
        "call = module.__call__.get_concrete_function(tf.TensorSpec(None, tf.float32))\n",
        "tf.saved_model.save(module, \"/tmp/module_with_signature\", signatures=call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lHiBm-kdKBmG"
      },
      "source": [
        "Notice that we first converted the `tf.function` to a `ConcreteFunction` with `get_concrete_function`. This is necessary because the function was created without a fixed `input_signature`, and so did not have a definite set of `Tensor` inputs associated with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nAzRHR0UT4hv"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir /tmp/module_with_signature --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0B25WsscTZoC"
      },
      "outputs": [],
      "source": [
        "imported = tf.saved_model.load(\"/tmp/module_with_signature\")\n",
        "signature = imported.signatures[\"serving_default\"]\n",
        "assert [3.] == signature(x=tf.constant([3.]))[\"output_0\"].numpy()\n",
        "imported.mutate(tf.constant(2.))\n",
        "assert [6.] == signature(x=tf.constant([3.]))[\"output_0\"].numpy()\n",
        "assert 2. == imported.v.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_gH91j1IR4tq"
      },
      "source": [
        "We exported a single signature, and its key defaulted to \"serving_default\". To export multiple signatures, pass a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6VYAiQmLUiox"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
        "def parse_string(string_input):\n",
        "  return imported(tf.strings.to_number(string_input))\n",
        "\n",
        "signatures = {\"serving_default\": parse_string,\n",
        "              \"from_float\": imported.signatures[\"serving_default\"]}\n",
        "\n",
        "tf.saved_model.save(imported, \"/tmp/module_with_multiple_signatures\", signatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8IPx_0RWEx07"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir /tmp/module_with_multiple_signatures --tag_set serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRdSPjKFfQpx"
      },
      "source": [
        "`saved_model_cli` can also run SavedModels directly from the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hcHmgVytfODo"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli run --dir /tmp/module_with_multiple_signatures --tag_set serve --signature_def serving_default --input_exprs=\"string_input='3.'\"\n",
        "!saved_model_cli run --dir /tmp/module_with_multiple_signatures --tag_set serve --signature_def from_float --input_exprs=\"x=3.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WiNhHa_Ne82K"
      },
      "source": [
        "## Fine-tuning imported models\n",
        "\n",
        "Variable objects are available, and we can backprop through imported functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mSchcIB2e-n0"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.optimizers.SGD(0.05)\n",
        "\n",
        "def train_step():\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = (10. - imported(tf.constant(2.))) ** 2\n",
        "  variables = tape.watched_variables()\n",
        "  grads = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(grads, variables))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Yx9bO2taJJxm"
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "  # \"v\" approaches 5, \"loss\" approaches 0\n",
        "  print(\"loss={:.2f} v={:.2f}\".format(train_step(), imported.v.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qyL9tOPrg5Zw"
      },
      "source": [
        "## Control flow in SavedModels\n",
        "\n",
        "Anything that can go in a `tf.function` can go in a SavedModel. With [AutoGraph](./autograph.ipynb) this includes conditional logic which depends on Tensors, specified with regular Python control flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tfbh3uGMgBpH"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec([], tf.int32)])\n",
        "def control_flow(x):\n",
        "  if x \u003c 0:\n",
        "    tf.print(\"Invalid!\")\n",
        "  else:\n",
        "    tf.print(x % 3)\n",
        "\n",
        "to_export = tf.Module()\n",
        "to_export.control_flow = control_flow\n",
        "tf.saved_model.save(to_export, \"/tmp/control_flow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bv4EXevIjHch"
      },
      "outputs": [],
      "source": [
        "imported = tf.saved_model.load(\"/tmp/control_flow\")\n",
        "imported.control_flow(tf.constant(-1))  # Invalid!\n",
        "imported.control_flow(tf.constant(2))   # 2\n",
        "imported.control_flow(tf.constant(3))   # 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dk5wWyuMpuHx"
      },
      "source": [
        "# SavedModels from Estimators\n",
        "\n",
        "Estimators export SavedModels through [`tf.Estimator.export_saved_model`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model). See the [guide to Estimator](https://www.tensorflow.org/guide/estimators) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "B9KQq5qzpzbK"
      },
      "outputs": [],
      "source": [
        "input_column = tf.feature_column.numeric_column(\"x\")\n",
        "estimator = tf.estimator.LinearClassifier(feature_columns=[input_column])\n",
        "\n",
        "def input_fn():\n",
        "  return tf.data.Dataset.from_tensor_slices(\n",
        "    ({\"x\": [1., 2., 3., 4.]}, [1, 1, 0, 0])).repeat(200).shuffle(64).batch(16)\n",
        "estimator.train(input_fn)\n",
        "\n",
        "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "  tf.feature_column.make_parse_example_spec([input_column]))\n",
        "export_path = estimator.export_saved_model(\n",
        "  \"/tmp/from_estimator/\", serving_input_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XJ4PJ-Cl4060"
      },
      "source": [
        "This SavedModel accepts serialized `tf.Example` protocol buffers, which are useful for serving. But we can also load it with `tf.saved_model.load` and run it from Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c_BUBBNB1UH9"
      },
      "outputs": [],
      "source": [
        "imported = tf.saved_model.load(export_path)\n",
        "\n",
        "def predict(x):\n",
        "  example = tf.train.Example()\n",
        "  example.features.feature[\"x\"].float_list.value.extend([x])\n",
        "  return imported.signatures[\"predict\"](\n",
        "    examples=tf.constant([example.SerializeToString()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "C1ylWZCQ1ahG"
      },
      "outputs": [],
      "source": [
        "print(predict(1.5))\n",
        "print(predict(3.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IrCCm0-isqA"
      },
      "source": [
        "`tf.estimator.export.build_raw_serving_input_receiver_fn` allows you to create input functions which take raw tensors rather than `tf.train.Example`s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Co6fDbzw_UnD"
      },
      "source": [
        "# Load a SavedModel in C++\n",
        "\n",
        "The C++ version of the SavedModel [loader](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.h) provides an API to load a SavedModel from a path, while allowing SessionOptions and RunOptions. You have to specify the tags associated with the graph to be loaded. The loaded version of SavedModel is referred to as SavedModelBundle and contains the MetaGraphDef and the session within which it is loaded.\n",
        "\n",
        "```C++\n",
        "const string export_dir = ...\n",
        "SavedModelBundle bundle;\n",
        "...\n",
        "LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},\n",
        "               \u0026bundle);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b33KuyEuAO3Z"
      },
      "source": [
        "\u003ca id=saved_model_cli/\u003e\n",
        "\n",
        "# Details of the SavedModel command line interface\n",
        "\n",
        "You can use the SavedModel Command Line Interface (CLI) to inspect and\n",
        "execute a SavedModel.\n",
        "For example, you can use the CLI to inspect the model's `SignatureDef`s.\n",
        "The CLI enables you to quickly confirm that the input\n",
        "Tensor dtype and shape match the model. Moreover, if you\n",
        "want to test your model, you can use the CLI to do a sanity check by\n",
        "passing in sample inputs in various formats (for example, Python\n",
        "expressions) and then fetching the output.\n",
        "\n",
        "\n",
        "### Install the SavedModel CLI\n",
        "\n",
        "Broadly speaking, you can install TensorFlow in either of the following\n",
        "two ways:\n",
        "\n",
        "*  By installing a pre-built TensorFlow binary.\n",
        "*  By building TensorFlow from source code.\n",
        "\n",
        "If you installed TensorFlow through a pre-built TensorFlow binary,\n",
        "then the SavedModel CLI is already installed on your system\n",
        "at pathname `bin\\saved_model_cli`.\n",
        "\n",
        "If you built TensorFlow from source code, you must run the following\n",
        "additional command to build `saved_model_cli`:\n",
        "\n",
        "```\n",
        "$ bazel build tensorflow/python/tools:saved_model_cli\n",
        "```\n",
        "\n",
        "### Overview of commands\n",
        "\n",
        "The SavedModel CLI supports the following two commands on a\n",
        "`MetaGraphDef` in a SavedModel:\n",
        "\n",
        "* `show`, which shows a computation on a `MetaGraphDef` in a SavedModel.\n",
        "* `run`, which runs a computation on a `MetaGraphDef`.\n",
        "\n",
        "\n",
        "### `show` command\n",
        "\n",
        "A SavedModel contains one or more `MetaGraphDef`s, identified by their tag-sets.\n",
        "To serve a model, you\n",
        "might wonder what kind of `SignatureDef`s are in each model, and what are their\n",
        "inputs and outputs.  The `show` command let you examine the contents of the\n",
        "SavedModel in hierarchical order.  Here's the syntax:\n",
        "\n",
        "```\n",
        "usage: saved_model_cli show [-h] --dir DIR [--all]\n",
        "[--tag_set TAG_SET] [--signature_def SIGNATURE_DEF_KEY]\n",
        "```\n",
        "\n",
        "For example, the following command shows all available\n",
        "MetaGraphDef tag-sets in the SavedModel:\n",
        "\n",
        "```\n",
        "$ saved_model_cli show --dir /tmp/saved_model_dir\n",
        "The given SavedModel contains the following tag-sets:\n",
        "serve\n",
        "serve, gpu\n",
        "```\n",
        "\n",
        "The following command shows all available `SignatureDef` keys in\n",
        "a `MetaGraphDef`:\n",
        "\n",
        "```\n",
        "$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve\n",
        "The given SavedModel `MetaGraphDef` contains `SignatureDefs` with the\n",
        "following keys:\n",
        "SignatureDef key: \"classify_x2_to_y3\"\n",
        "SignatureDef key: \"classify_x_to_y\"\n",
        "SignatureDef key: \"regress_x2_to_y3\"\n",
        "SignatureDef key: \"regress_x_to_y\"\n",
        "SignatureDef key: \"regress_x_to_y2\"\n",
        "SignatureDef key: \"serving_default\"\n",
        "```\n",
        "\n",
        "If a `MetaGraphDef` has *multiple* tags in the tag-set, you must specify\n",
        "all tags, each tag separated by a comma. For example:\n",
        "\n",
        "\u003cpre\u003e\n",
        "$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve,gpu\n",
        "\u003c/pre\u003e\n",
        "\n",
        "To show all inputs and outputs TensorInfo for a specific `SignatureDef`, pass in\n",
        "the `SignatureDef` key to `signature_def` option. This is very useful when you\n",
        "want to know the tensor key value, dtype and shape of the input tensors for\n",
        "executing the computation graph later. For example:\n",
        "\n",
        "```\n",
        "$ saved_model_cli show --dir \\\n",
        "/tmp/saved_model_dir --tag_set serve --signature_def serving_default\n",
        "The given SavedModel SignatureDef contains the following input(s):\n",
        "  inputs['x'] tensor_info:\n",
        "      dtype: DT_FLOAT\n",
        "      shape: (-1, 1)\n",
        "      name: x:0\n",
        "The given SavedModel SignatureDef contains the following output(s):\n",
        "  outputs['y'] tensor_info:\n",
        "      dtype: DT_FLOAT\n",
        "      shape: (-1, 1)\n",
        "      name: y:0\n",
        "Method name is: tensorflow/serving/predict\n",
        "```\n",
        "\n",
        "To show all available information in the SavedModel, use the `--all` option.\n",
        "For example:\n",
        "\n",
        "\u003cpre\u003e\n",
        "$ saved_model_cli show --dir /tmp/saved_model_dir --all\n",
        "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
        "\n",
        "signature_def['classify_x2_to_y3']:\n",
        "  The given SavedModel SignatureDef contains the following input(s):\n",
        "    inputs['inputs'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: x2:0\n",
        "  The given SavedModel SignatureDef contains the following output(s):\n",
        "    outputs['scores'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: y3:0\n",
        "  Method name is: tensorflow/serving/classify\n",
        "\n",
        "...\n",
        "\n",
        "signature_def['serving_default']:\n",
        "  The given SavedModel SignatureDef contains the following input(s):\n",
        "    inputs['x'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: x:0\n",
        "  The given SavedModel SignatureDef contains the following output(s):\n",
        "    outputs['y'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: y:0\n",
        "  Method name is: tensorflow/serving/predict\n",
        "\u003c/pre\u003e\n",
        "\n",
        "\n",
        "### `run` command\n",
        "\n",
        "Invoke the `run` command to run a graph computation, passing\n",
        "inputs and then displaying (and optionally saving) the outputs.\n",
        "Here's the syntax:\n",
        "\n",
        "```\n",
        "usage: saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
        "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
        "                           [--input_exprs INPUT_EXPRS]\n",
        "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
        "                           [--overwrite] [--tf_debug]\n",
        "```\n",
        "\n",
        "The `run` command provides the following three ways to pass inputs to the model:\n",
        "\n",
        "* `--inputs` option enables you to pass numpy ndarray in files.\n",
        "* `--input_exprs` option enables you to pass Python expressions.\n",
        "* `--input_examples` option enables you to pass \u003ca href=\"./../api_docs/python/tf/train/Example\"\u003e\u003ccode\u003etf.train.Example\u003c/code\u003e\u003c/a\u003e.\n",
        "\n",
        "#### `--inputs`\n",
        "\n",
        "To pass input data in files, specify the `--inputs` option, which takes the\n",
        "following general format:\n",
        "\n",
        "```bsh\n",
        "--inputs \u003cINPUTS\u003e\n",
        "```\n",
        "\n",
        "where *INPUTS* is either of the following formats:\n",
        "\n",
        "*  `\u003cinput_key\u003e=\u003cfilename\u003e`\n",
        "*  `\u003cinput_key\u003e=\u003cfilename\u003e[\u003cvariable_name\u003e]`\n",
        "\n",
        "You may pass multiple *INPUTS*. If you do pass multiple inputs, use a semicolon\n",
        "to separate each of the *INPUTS*.\n",
        "\n",
        "`saved_model_cli` uses `numpy.load` to load the *filename*.\n",
        "The *filename* may be in any of the following formats:\n",
        "\n",
        "*  `.npy`\n",
        "*  `.npz`\n",
        "*  pickle format\n",
        "\n",
        "A `.npy` file always contains a numpy ndarray. Therefore, when loading from\n",
        "a `.npy` file, the content will be directly assigned to the specified input\n",
        "tensor. If you specify a *variable_name* with that `.npy` file, the\n",
        "*variable_name* will be ignored and a warning will be issued.\n",
        "\n",
        "When loading from a `.npz` (zip) file, you may optionally specify a\n",
        "*variable_name* to identify the variable within the zip file to load for\n",
        "the input tensor key.  If you don't specify a *variable_name*, the SavedModel\n",
        "CLI will check that only one file is included in the zip file and load it\n",
        "for the specified input tensor key.\n",
        "\n",
        "When loading from a pickle file, if no `variable_name` is specified in the\n",
        "square brackets, whatever that is inside the pickle file will be passed to the\n",
        "specified input tensor key. Otherwise, the SavedModel CLI will assume a\n",
        "dictionary is stored in the pickle file and the value corresponding to\n",
        "the *variable_name* will be used.\n",
        "\n",
        "\n",
        "#### `--input_exprs`\n",
        "\n",
        "To pass inputs through Python expressions, specify the `--input_exprs` option.\n",
        "This can be useful for when you don't have data\n",
        "files lying around, but still want to sanity check the model with some simple\n",
        "inputs that match the dtype and shape of the model's `SignatureDef`s.\n",
        "For example:\n",
        "\n",
        "```bsh\n",
        "`\u003cinput_key\u003e=[[1],[2],[3]]`\n",
        "```\n",
        "\n",
        "In addition to Python expressions, you may also pass numpy functions. For\n",
        "example:\n",
        "\n",
        "```bsh\n",
        "`\u003cinput_key\u003e=np.ones((32,32,3))`\n",
        "```\n",
        "\n",
        "(Note that the `numpy` module is already available to you as `np`.)\n",
        "\n",
        "\n",
        "#### `--input_examples`\n",
        "\n",
        "To pass `tf.train.Example` as inputs, specify the `--input_examples` option.\n",
        "For each input key, it takes a list of dictionary, where each dictionary is an\n",
        "instance of `tf.train.Example`. The dictionary keys are the features and the\n",
        "values are the value lists for each feature.\n",
        "For example:\n",
        "\n",
        "```bsh\n",
        "`\u003cinput_key\u003e=[{\"age\":[22,24],\"education\":[\"BS\",\"MS\"]}]`\n",
        "```\n",
        "\n",
        "#### Save output\n",
        "\n",
        "By default, the SavedModel CLI writes output to stdout. If a directory is\n",
        "passed to `--outdir` option, the outputs will be saved as npy files named after\n",
        "output tensor keys under the given directory.\n",
        "\n",
        "Use `--overwrite` to overwrite existing output files.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "saved_model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
