{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2.0: Train and save a model",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "I9gUzvnVPCoy"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "I9gUzvnVPCoy"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\")."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wuUgEPFW9-V7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Vm7hH0EC97mD"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow 2.0: Train and save a model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SW83ZEZg8BN5"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/2/guide/train_and_save\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/train_and_save.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/train_and_save.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NRXR0hJKPCo2"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook trains a simple MNIST model to demonstrate the basic workflow for using TensorFlow 2.0 APIs:\n",
        "\n",
        "1. Define a model\n",
        "2. Preprocessing your data into a `tf.data.Dataset`\n",
        "3. Train the model with the dataset\n",
        "  - Use `tf.GradientTape` to compute gradients\n",
        "  - Use stateful `tf.keras.metrics.*` to collect metrics of interest\n",
        "  - Log metrics with `tf.summary.*` APIs to view in TensorBoard\n",
        "  - Use `tf.train.Checkpoint` to save and restore weights\n",
        "4. Export a `SavedModel` with `tf.saved_model` (this is a portable representation of the model that can be imported into C++, JS, Python without knowledge of the original TensorFlow code.)\n",
        "5. Re-import the `SavedModel` and demonstrate its usage in Python."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vNM_jwND8-PY"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Import TensorFlow 2.0 Preview Nightly and enable TF 2.0 mode:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LPYWfsC09BJU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z-T4T8IEoQRf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yo5oaJ-hAnk_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops import summary_ops_v2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "c3iafiz9PCpA"
      },
      "cell_type": "markdown",
      "source": [
        "## Define a model with the tf.Keras API\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PtOPPajN6WI3"
      },
      "cell_type": "markdown",
      "source": [
        "Build a convolutional model using the [tf.Keras API](https://www.tensorflow.org/guide/keras). This model uses the `channel_last` [data format](https://www.tensorflow.org/guide/performance/overview#data_formats)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z_cEHfTdPCpB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def create_model():\n",
        "  max_pool = layers.MaxPooling2D((2, 2), (2, 2), padding='same')\n",
        "  # The model consists of a sequential chain of layers, so tf.keras.Sequential\n",
        "  # (a subclass of tf.keras.Model) makes for a compact description.\n",
        "  return tf.keras.Sequential([\n",
        "      layers.Reshape(\n",
        "          target_shape=[28, 28, 1],\n",
        "          input_shape=(28, 28,)),\n",
        "      layers.Conv2D(2, 5, padding='same', activation=tf.nn.relu),\n",
        "      max_pool,\n",
        "      layers.Conv2D(4, 5, padding='same', activation=tf.nn.relu),\n",
        "      max_pool,\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(32, activation=tf.nn.relu),\n",
        "      layers.Dropout(0.4),\n",
        "      layers.Dense(10)])\n",
        "\n",
        "\n",
        "# Define a loss function and accuracy function\n",
        "def compute_loss(logits, labels):\n",
        "  return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits))\n",
        "\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "  return tf.keras.metrics.categorical_accuracy(labels, logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UY2CZgNW7_Jy"
      },
      "cell_type": "markdown",
      "source": [
        "Create the model and optimizer:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YsaXPR6OPCpE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7bgV8B1wPCpJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and create datasets\n",
        "\n",
        "Load the MNIST dataset into a [tf.data.Dataset](https://www.tensorflow.org/guide/datasets). This provides useful transformations like batching and shuffling.\n",
        "\n",
        "Note: Keras models can train directly on numpy arrays for small datasets (see [basic classification](../keras/basic_classification.ipynb)). The use of `tf.data` here is to demonstrate the API for applications that need more scalability.   "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zxmeEGyhPCpL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up datasets\n",
        "def mnist_datasets():\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  # Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.\n",
        "  x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)\n",
        "  y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "  return train_dataset, test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YtB65OO0PCpP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = mnist_datasets()\n",
        "train_ds = train_ds.shuffle(60000).batch(100)\n",
        "test_ds = test_ds.batch(100)\n",
        "\n",
        "print('Dataset will yield tensors of the following shape: {}'.format(train_ds.output_shapes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uB_C1r9PPCpU"
      },
      "cell_type": "markdown",
      "source": [
        "## Configure training\n",
        "\n",
        "Note: Keras models include a complete training loop (see [basic classification](../keras/basic_classification.ipynb)). The training process is only defined manually here as a starting point for applications that need deeper customization.   \n",
        "\n",
        "The `train()` function iterates over the training dataset, computing the gradients for each batch and then applying them to the model variables. It periodically outputs summaries."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G8EfprJ1PCpU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(model, optimizer, images, labels):\n",
        "  # Record the operations used to compute the loss, so that the gradient\n",
        "  # of the loss with respect to the variables can be computed.\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images, training=True)\n",
        "    loss = compute_loss(logits, labels)\n",
        "    accuracy = compute_accuracy(logits, labels)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  return loss, accuracy\n",
        "\n",
        "\n",
        "def train(model, optimizer, dataset, log_freq=50):\n",
        "  \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
        "  start = time.time()\n",
        "  # Metrics are stateful. They accumulate values and return a cumulative\n",
        "  # result when you call .result(). Clear accumulated values with .reset_states()\n",
        "  avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "  avg_accuracy = tf.keras.metrics.Mean('accuracy', dtype=tf.float32)\n",
        "  # Datasets can be iterated over like any other Python iterable.\n",
        "  for images, labels in dataset:\n",
        "    loss, accuracy = train_step(model, optimizer, images, labels)\n",
        "    avg_loss(loss)\n",
        "    avg_accuracy(accuracy)\n",
        "    if tf.equal(optimizer.iterations % log_freq, 0):\n",
        "      summary_ops_v2.scalar('loss', avg_loss.result(), step=optimizer.iterations)\n",
        "      summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=optimizer.iterations)\n",
        "      avg_loss.reset_states()\n",
        "      avg_accuracy.reset_states()\n",
        "      rate = log_freq / (time.time() - start)\n",
        "      print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (optimizer.iterations, loss, rate))\n",
        "      start = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6zrAPkdEPCpa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, dataset, step_num):\n",
        "  \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
        "  avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "  avg_accuracy = tf.keras.metrics.Mean('accuracy', dtype=tf.float32)\n",
        "\n",
        "  for (images, labels) in dataset:\n",
        "    logits = model(images, training=False)\n",
        "    avg_loss(compute_loss(logits, labels))\n",
        "    avg_accuracy(compute_accuracy(logits, labels))\n",
        "  print('Model test set loss: {:0.4f} accuracy: {:0.2f}%'.format(\n",
        "      avg_loss.result(), avg_accuracy.result() * 100))\n",
        "  summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
        "  summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=step_num)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kox8FEeNPCpd"
      },
      "cell_type": "markdown",
      "source": [
        "## Configure model directory\n",
        "\n",
        "Use one directory to save the relevant artifactsâ€”summary logs, checkpoints, and `SavedModel` exports."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bLwFfkYhPCpe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Where to save checkpoints, tensorboard summaries, etc.\n",
        "MODEL_DIR = '/tmp/tensorflow/mnist'\n",
        "\n",
        "\n",
        "def apply_clean():\n",
        "  if tf.io.gfile.exists(MODEL_DIR):\n",
        "    print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
        "    tf.io.gfile.rmtree(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5DUL7OVYPCph",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Optional: wipe the existing directory\n",
        "apply_clean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SL51Mdg9PCpj"
      },
      "cell_type": "markdown",
      "source": [
        "You can configure the output location for the training summaries. Previously, we called `tf.summary.scalar(...)` in the `train()` function, by using the `summary_writer` in a `with` block, you can catch those generated summaries and direct them to a file. View the summaries with `tensorboard --logdir=<model_dir>`"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YZgxx95-PCpk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\n",
        "test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\n",
        "\n",
        "train_summary_writer = summary_ops_v2.create_file_writer(\n",
        "  train_dir, flush_millis=10000)\n",
        "test_summary_writer = summary_ops_v2.create_file_writer(\n",
        "  test_dir, flush_millis=10000, name='test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pLSsDST8PCpn"
      },
      "cell_type": "markdown",
      "source": [
        "## Configure checkpoints\n",
        "\n",
        "The `tf.train.Checkpoint` object helps manage which `tf.Variable`s are saved and restored from the checkpoint files.\n",
        "\n",
        "A checkpoint differs from a `SavedModel` because it additionally keeps track of training-related state, such as momentum variables for a momentum-based optimizer or things like the global step. A checkpoint only stores weights so you'll need the original code to define the computation with those weights."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Xab3feHXPCpp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "  model=model, optimizer=optimizer)\n",
        "\n",
        "# Restore variables on creation if a checkpoint exists.\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "U0ZNI7rFPCps"
      },
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Now that `train()` and `test()` are set up, create a model and train it for some number of epochs:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CagzdcoRFrN9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_TRAIN_EPOCHS = 1\n",
        "\n",
        "for i in range(NUM_TRAIN_EPOCHS):\n",
        "  start = time.time()\n",
        "  with train_summary_writer.as_default():\n",
        "    train(model, optimizer, train_ds)\n",
        "  end = time.time()\n",
        "  print('\\nTrain time for epoch #{} ({} total steps): {}'.format(\n",
        "      i + 1, optimizer.iterations, end - start))\n",
        "  with test_summary_writer.as_default():\n",
        "    test(model, test_ds, optimizer.iterations)\n",
        "  checkpoint.save(checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dc1hV_21PCpw"
      },
      "cell_type": "markdown",
      "source": [
        "## Export a SavedModel"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NGCnOqMTPCpy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "export_path = os.path.join(MODEL_DIR, 'export')\n",
        "\n",
        "tf.saved_model.save(model, export_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "uECIhO7aPCp1"
      },
      "cell_type": "markdown",
      "source": [
        "## Restore and run the SavedModel\n",
        "\n",
        "Restore any `SavedModel` and call it without reference to the original source code. APIs for importing and transforming `SavedModel`s exist for a variety of languages. See the [SavedModel guide](https://www.tensorflow.org/guide/saved_model) for more."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "go-t0CvMzrSi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def import_and_eval():\n",
        "  restored_model = tf.saved_model.restore(export_path)\n",
        "  _, (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  x_test = x_test / np.float32(255)\n",
        "  y_predict = restored_model(x_test)\n",
        "  accuracy = compute_accuracy(y_predict, y_test)\n",
        "  print('Model accuracy: {:0.2f}%'.format(accuracy.numpy() * 100))\n",
        "\n",
        "# TODO(brianklee): Activate after v2 import is implemented.\n",
        "# import_and_eval()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}