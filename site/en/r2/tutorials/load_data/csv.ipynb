{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DweYe9FcbMK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVV2e0XKbJeX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtoed20cRJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load text with tf.data"
      ]
    },
    {
      "metadata": {
        "id": "1ap_W4aQcgNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "NWeQAo0Ec_BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial provides an example of how to use `tf.data.TextLineDataset` to load examples from text files. `TextLineDataset` is designed to create a dataset from a text file, in which each example is a line of text from the original file. This is potentially useful for any text data that is primarily line-based (for example, poetry or error logs).\n",
        "\n",
        "In this tutorial, we'll use three different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text."
      ]
    },
    {
      "metadata": {
        "id": "fgZ9gjmPfSnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "I4dwMQVQMQWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "baYFZMW_bJHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals \n",
        "\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWVWjyIkffau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The texts of the three translations are by:\n",
        "\n",
        " - [William Cowper](https://en.wikipedia.org/wiki/William_Cowper) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
        " \n",
        " - [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
        " \n",
        "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
        "\n",
        "The text files used in this tutorial have undergone some typical preprocessing tasks, mostly removing stuff — document header and footer, line numbers, chapter titles. Download these lightly munged files locally. "
      ]
    },
    {
      "metadata": {
        "id": "hRI0ZlhTgQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  r = requests.get(\"\".join([DIRECTORY_URL, name]), allow_redirects=True)\n",
        "  with open(name, 'wb') as f:\n",
        "    f.write(r.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54Dv7mCrf9Yw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3sDy6nuXoNp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load text into datasets\n",
        "\n",
        "Iterate through the files, loading each one into its own dataset.\n",
        "\n",
        "Each example needs to be labeled individually labeled, so use `tf.data.Dataset.map` to apply a labeler function to each one. This will iterate over every example in the dataset, returning (`example, label`) pairs."
      ]
    },
    {
      "metadata": {
        "id": "K0BjCOpOh7Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labeler(label_int):\n",
        "  \"\"\"Returns a labeler function initialized with a specfic label.\"\"\"\n",
        "  label_tensor = tf.cast(label_int, tf.int64)\n",
        "\n",
        "  def labeler(example):\n",
        "    \"\"\"Returns a labeled example.\"\"\"\n",
        "    return example, label_tensor\n",
        "  \n",
        "  return labeler\n",
        "\n",
        "labeled_data_sets = []\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(file_name)\n",
        "  labeler_function = get_labeler(i)\n",
        "  labeled_dataset = lines_dataset.map(labeler_function)\n",
        "  labeled_data_sets.append(labeled_dataset)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8PHK5J_cXE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Combine these labeled datasets into a single dataset, and shuffle it.\n"
      ]
    },
    {
      "metadata": {
        "id": "Qd544E-Sh63L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_labeled_data = labeled_data_sets[0].concatenate(labeled_data_sets[1])\n",
        "all_labeled_data = all_labeled_data.concatenate(labeled_data_sets[2])\n",
        "\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "                    tf.cast(50000, tf.int64), \n",
        "                    reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4JEHrJXeG5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can use `tf.data.Dataset.take` and `print` to see what the `(example, label)` pairs look like. The `numpy` property shows each Tensor's value."
      ]
    },
    {
      "metadata": {
        "id": "gywKlN0xh6u5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rrpU2_sfDh0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encode text lines as numbers\n",
        "\n",
        "Machine learning models work on numbers, not words, so the string values need to be converted into lists of numbers. To do that, map each unique word to a unique integer.\n",
        "\n",
        "### Build vocabulary\n",
        "\n",
        "First, build a vocabulary by tokenizing the text into a collection of individual unique words. There are a few ways to do this in both TensorFlow and Python. For this tutorial:\n",
        "\n",
        "1. Iterate over each example's `numpy` value.\n",
        "2. Use `tfds.features.text.Tokenizer` to split it into tokens.\n",
        "3. Collect these tokens into a Python set, to remove duplicates.\n",
        "4. Get the size of the vocabulary for later use."
      ]
    },
    {
      "metadata": {
        "id": "YkHtbGnDh6mg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in all_labeled_data:\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0W35VJqAh9zs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode examples\n",
        "\n",
        "Create an encoder by passing the `vocabulary_set` to `tfds.features.text.TokenTextEncoder`. The encoder's `encode` method takes in a string of text and returns a list of integers."
      ]
    },
    {
      "metadata": {
        "id": "gkxJIVAth6j0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v6S5Qyabi-vo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can try this on a single line to see what the output looks like."
      ]
    },
    {
      "metadata": {
        "id": "jgxPZaxUuTbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_text = next(iter(all_labeled_data))[0].numpy()\n",
        "example_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XoVpKR3qj5yb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_example = encoder.encode(example_text)\n",
        "encoded_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p9qHM0v8k_Mg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now run the encoder on the dataset by wrapping it in `tf.py_function` and  passing that to the dataset's `map` method."
      ]
    },
    {
      "metadata": {
        "id": "HcIQ7LOTh6eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "  return tf.py_function(\n",
        "    encode, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
        "  )\n",
        "\n",
        "all_encoded_data = all_labeled_data.map(encode_map_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YZToSXSm0qr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into text and train batches\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to create a small test dataset and a larger training set.\n",
        "\n",
        "Before being passed into the model, the datasets need to be batched. Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
      ]
    },
    {
      "metadata": {
        "id": "r-rmbijQh6bf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = all_encoded_data.take(5000).padded_batch(50, padded_shapes=([-1],[]))\n",
        "train_data = all_encoded_data.skip(5000).padded_batch(50, padded_shapes=([-1],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xdz7SVwmqi1l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, `test_data` and `train_data` are not collections of (`example, label`) pairs, but collections of batches. Each batch is a pair of (*many examples*, *many labels*) represented as arrays.\n",
        "\n",
        "To illustrate:"
      ]
    },
    {
      "metadata": {
        "id": "kMslWfuwoqpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one_batch = next(iter(test_data))\n",
        "\n",
        "\n",
        "# a single text line, out of many in a batch\n",
        "# notice the zero values padding out the list\n",
        "one_batch[0].numpy()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AFLV89xfrrIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# a bunch of labels\n",
        "one_batch[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UI4I6_Sa0vWu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we have introduced a new token encoding (the zero used for padding), the vocabulary size has increased by one."
      ]
    },
    {
      "metadata": {
        "id": "IlD1Lli91vuc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8SUhGFNsmRi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G9gHq2Ii2Krm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For simplicity's sake, start with a `tf.keras.Sequential` model."
      ]
    },
    {
      "metadata": {
        "id": "QJgI1pow2YR9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6whpE1Rv2cF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The input is collection of integers representing text tokens (words). There are two downsides to this representation:\n",
        "\n",
        "*  The integer-encoding is arbitrary. Therfore it does not capture any information about the words.\n",
        "\n",
        "*  An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because different words may have a similar encoding, this feature-weight combination is not meaningful.\n",
        "\n",
        "To overcome these challenges, the first layer of the model will be a `tf.keras.laters.Embedding` layer. This transforms each integer into a dense collection of floating point values. The exact encoding of integers to collections of floats is learned during training, so the encodings become meaningful to the model, rather than arbitrary integer assignments.\n",
        "\n",
        "When creating an embedding layer, the two required arguments are:\n",
        "\n",
        "*   input dimensionality — the number of integers appearing in the input set (that is, the size of the vocabulary)\n",
        "*   embedding dimension — the number of floating point values that will represent a single integer (higher values discover more complex relationships between words, but require more data)\n",
        "\n",
        "See the [Word Embeddings](../../tutorials/sequences/word_embeddings) tutorial for more details."
      ]
    },
    {
      "metadata": {
        "id": "DR6-ctbY638P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZ51qHRL6514",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a bidirectional [Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory) layer by wrapping `tf.keras.layers.LSTM` in `tf.keras.layers.Birectional`. An LSTM is a type of Recurrent Neural Network that allows the model to understand data points in relationship to the datapoints that came before it. That is, a word in the context of a sentence, rather than a word in isolation.\n",
        "\n",
        "To see the effect of the LSTM, you can try building the model with and without it, or try replacing it with an additional dense layer. "
      ]
    },
    {
      "metadata": {
        "id": "x6rnq6DN_WUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2ahFYPH_X_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally we'll have a series of one or more densely connected layers, with the last one being the output layer.\n",
        "\n",
        "The output layer produces a one-hot encoding of one of three values (the labels representing the three different translators). Use the `softmax` activation to output three probability values. The node with the highest probability is the models prediction of an example's label.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QTEaNSnLCsv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One or more dense layers.\n",
        "# Edit the list in the `for` line to experiment with layer sizes.\n",
        "for units in [64, 64]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "\n",
        "# Output layer. The first argument is the number of labels.\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLHPU8q5DLi_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. You can try other optimizers, but `adam` is very common."
      ]
    },
    {
      "metadata": {
        "id": "pkTBUVO4h6Y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DM-HLo5NDhql",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "This model running on this data produces decent results (about 83%) after just two epochs and levels off quickly. A third epoch lowers accuracy by a tiny amount.\n",
        "\n",
        "If you edit the model, try running several epochs to see when improvement levels off and overfitting sets in. To check progress after each epoch, validate against the `test_data` set. (This is the same as running `model.evaluate(test_data)` after each epoch.)"
      ]
    },
    {
      "metadata": {
        "id": "aLtO33tNh6V8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_data, epochs=3, validation_data=test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KTPCYf_Jh6TH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}