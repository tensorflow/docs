{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISubpr_SSsiM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3jTMb1dySr3V"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6DWfyNThSziV"
      },
      "source": [
        "# tf.function\n",
        "\n",
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/eager/tf_function\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/eager/tf_function.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/eager/tf_function.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J122XQYG7W6w"
      },
      "source": [
        "\n",
        "In TensorFlow 2.0 eager execution is turned on by default. This gets you a very\n",
        "intuitive and flexible user interface (running one-off operations is much easier\n",
        "and faster) but this can come at the expense of performance and deployability.\n",
        "\n",
        "To get peak performance and to make your model deployable anywhere, we provide\n",
        "`tf.function` as the tool you can use to make graphs out of your programs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "otIdN1TS8N7S"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install tensorflow==2.0.0-alpha0\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SbtT1-Wm70F2"
      },
      "outputs": [],
      "source": [
        "# A function is like an op\n",
        "\n",
        "@tf.function\n",
        "def add(a, b):\n",
        "  return a + b\n",
        "\n",
        "add(tf.ones([2, 2]), tf.ones([2, 2]))  #  [[2., 2.], [2., 2.]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bfFQfPGy73oe"
      },
      "source": [
        "A `tf.function` you define is just like a core TensorFlow operation: you can execute it eagerly, you can use it in a graph, it has gradients, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uP-zUelB8DbX"
      },
      "outputs": [],
      "source": [
        "# Functions have gradients\n",
        "\n",
        "@tf.function\n",
        "def add(a, b):\n",
        "  return a + b\n",
        "\n",
        "v = tf.Variable(1.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  result = add(v, 1.0)\n",
        "tape.gradient(result, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "l5qRjdbBVdU6"
      },
      "outputs": [],
      "source": [
        "# You can use functions inside functions\n",
        "\n",
        "@tf.function\n",
        "def dense_layer(x, w, b):\n",
        "  return add(tf.matmul(x, w), b)\n",
        "\n",
        "dense_layer(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uZ4Do2AV80cO"
      },
      "source": [
        "# Polymorphism\n",
        "\n",
        "`tf.function` tries to be as generic as a Python function. You can call Python functions with all sorts of signatures, and Python will usually do something reasonable. `tf.function` does this type of polymorphism for you even though the underlying TensorFlow graphs it generates are specific to the particular types in its signature.\n",
        "\n",
        "You can call a function with arguments of different types to see what is happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kojmJrgq8U9v"
      },
      "outputs": [],
      "source": [
        "# Functions are polymorphic\n",
        "\n",
        "@tf.function\n",
        "def add(a):\n",
        "  return a + a\n",
        "\n",
        "print(\"add 1\", add(1))\n",
        "print(\"add 1.1\", add(1.1))\n",
        "print(\"add string tensor\", add(tf.constant(\"a\")))\n",
        "c = add.get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.string))\n",
        "c(a=tf.constant(\"a\"))  # aa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FRp7IjWFWECa"
      },
      "outputs": [],
      "source": [
        "# Functions can be faster than eager code, for graphs with many small ops\n",
        "\n",
        "import timeit\n",
        "conv_layer = tf.keras.layers.Conv2D(100, 3)\n",
        "\n",
        "@tf.function\n",
        "def conv_fn(image):\n",
        "  return conv_layer(image)\n",
        "\n",
        "image = tf.zeros([1, 200, 200, 100])\n",
        "# warm up\n",
        "conv_layer(image); conv_fn(image)\n",
        "print(\"Eager conv:\", timeit.timeit(lambda: conv_layer(image), number=10))\n",
        "print(\"Function conv:\", timeit.timeit(lambda: conv_fn(image), number=10))\n",
        "print(\"Note how there's not much difference in performance for convolutions\")\n",
        "\n",
        "lstm_cell = tf.keras.layers.LSTMCell(10)\n",
        "\n",
        "@tf.function\n",
        "def lstm_fn(input, state):\n",
        "  return lstm_cell(input, state)\n",
        "\n",
        "input = tf.zeros([10, 10])\n",
        "state = [tf.zeros([10, 10])] * 2\n",
        "# warm up\n",
        "lstm_cell(input, state); lstm_fn(input, state)\n",
        "print(\"eager lstm:\", timeit.timeit(lambda: lstm_cell(input, state), number=10))\n",
        "print(\"function lstm:\", timeit.timeit(lambda: lstm_fn(input, state), number=10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tRdlnCfV_UTn"
      },
      "source": [
        "## State in `tf.function`\n",
        "\n",
        "A very appealing property of functions as the programming model, over a general dataflow graph, is that functions can give the runtime more information about what was the intended behavior of the code.\n",
        "\n",
        "For example, when writing code which has multiple reads and writes to the same variables, a dataflow graph might not naturally encode the originally intended order of operations. In `tf.function`, however, because we're converting code which was traced from Python, we know the intended execution order.\n",
        "\n",
        "This means there's no need to add manual control dependencies; `tf.function` is smart enough to add the minimal set of necessary and sufficient control dependencies for your code to run correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SASm0ss8erVX"
      },
      "outputs": [],
      "source": [
        "# Automatic control dependencies\n",
        "\n",
        "a = tf.Variable(1.0)\n",
        "b = tf.Variable(2.0)\n",
        "\n",
        "@tf.function\n",
        "def f(x, y):\n",
        "  a.assign(y * b)\n",
        "  b.assign_add(x * a)\n",
        "  return a + b\n",
        "\n",
        "f(1.0, 2.0)  # 10.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lPr_6mK_AQWL"
      },
      "source": [
        "## Variables\n",
        "\n",
        "We can use the same idea of leveraging the intended execution order of the code to make variable creation and utilization very easy in `tf.function`. There is one very important caveat, though, which is that with variables it's possible to write code which behaves different when called eagerly multiple times and when its output tensor is evaluated multiple times.\n",
        "\n",
        "Here is a simple example:\n",
        "```python\n",
        "@tf.function\n",
        "def f(x):\n",
        "  v = tf.Variable(1.0)\n",
        "  v.assign_add(x)\n",
        "  return v\n",
        "\n",
        "f(1.) # Note: BROKEN, will throw exception\n",
        "```\n",
        "\n",
        "If you run this with eager execution, you'll always get \"2\" as the answer; but if you repeatedly evaluate the Tensor obtained from `f(1.)` in a graph context you'll get increasing numbers.\n",
        "\n",
        "So `tf.function` does not allow you to write code like that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DKzNjVg8h4ao"
      },
      "outputs": [],
      "source": [
        "# Non-ambiguous code is ok though\n",
        "\n",
        "v = tf.Variable(1.0)\n",
        "\n",
        "@tf.function\n",
        "def f(x):\n",
        "  return v.assign_add(x)\n",
        "\n",
        "f(1.0)  # 2.0\n",
        "f(2.0)  # 4.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HQrG5_kOiKl_"
      },
      "outputs": [],
      "source": [
        "# You can also create variables inside a tf.function as long as we can prove\n",
        "# that those variables are created only the first time the function is executed.\n",
        "\n",
        "class C: pass\n",
        "obj = C(); obj.v = None\n",
        "\n",
        "@tf.function\n",
        "def g(x):\n",
        "  if obj.v is None:\n",
        "    obj.v = tf.Variable(1.0)\n",
        "  return obj.v.assign_add(x)\n",
        "\n",
        "g(1.0)  # 2.0\n",
        "g(2.0)  # 4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_IOVc1eujMH2"
      },
      "outputs": [],
      "source": [
        "# Variable initializers can depend on function arguments and on values of other\n",
        "# variables. We can figure out the right initialization order using the same\n",
        "# method we use to generate control dependencies.\n",
        "\n",
        "state = []\n",
        "@tf.function\n",
        "def fn(x):\n",
        "  if not state:\n",
        "    state.append(tf.Variable(2.0 * x))\n",
        "    state.append(tf.Variable(state[0] * 3.0))\n",
        "  return state[0] * x * state[1]\n",
        "\n",
        "fn(tf.constant(1.0))\n",
        "fn(tf.constant(3.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5f05Vr_YBUCz"
      },
      "source": [
        "## Control flow and autograph\n",
        "\n",
        "While `tf.cond` and `tf.while_loop` continue to work with `tf.function`, we provide a better alternative based on lightweight compilation of your Python code.\n",
        "\n",
        "The [autograph](https://www.tensorflow.org/guide/autograph) library is fully integrated with `tf.function`, and it will rewrite conditionals and loops which depend on Tensors to run dynamically in the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yCQTtTPTW3WF"
      },
      "outputs": [],
      "source": [
        "# Simple loop\n",
        "\n",
        "@tf.function\n",
        "def f(x):\n",
        "  while tf.reduce_sum(x) \u003e 1:\n",
        "    tf.print(x)\n",
        "    x = tf.tanh(x)\n",
        "  return x\n",
        "\n",
        "f(tf.random.uniform([10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jlQD1ffRXJhl"
      },
      "outputs": [],
      "source": [
        "# If you're curious you can inspect the code autograph generates.\n",
        "# It feels like reading assembly language, though.\n",
        "\n",
        "def f(x):\n",
        "  while tf.reduce_sum(x) \u003e 1:\n",
        "    tf.print(x)\n",
        "    x = tf.tanh(x)\n",
        "  return x\n",
        "\n",
        "print(tf.autograph.to_code(f))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CNqGBVJXCVKU"
      },
      "source": [
        "To control autograph, remember that it only affects the basic control flow constructs in Python (if, for, while, break, etc) and that it only changes them if the predicates are Tensors.\n",
        "\n",
        "So in the following example the first loop is statically unrolled while the second loop is dynamically converted:\n",
        "\n",
        "```python\n",
        "@tf.function\n",
        "def f(x):\n",
        "  for i in range(10):  # Static python loop, we'll not convert it\n",
        "    do_stuff()\n",
        "  for i in tf.range(10):  # depends on a tensor, we'll convert it\n",
        "```\n",
        "\n",
        "Similarly, to guarantee that prints and asserts happen dynamically, use `tf.print` and `tf.assert`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lDpuZLL2emjP"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def f(x):\n",
        "  for i in tf.range(10):\n",
        "    tf.print(i)\n",
        "    tf.Assert(i \u003c 10, [\"a\"])\n",
        "    x += x\n",
        "  return x\n",
        "\n",
        "f(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hyksHW9TCukR"
      },
      "source": [
        "Finally, autograph cannot compile arbitrary Python code into TensorFlow graphs. Specifically, the data structures which you use dynamically still need to be TensorFlow data structures.\n",
        "\n",
        "So, for example, the best way to accumulate data in a loop is still to use `tf.TensorArray`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HJ3Vb3dXfefN"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def f(x):\n",
        "  ta = tf.TensorArray(tf.float32, size=10)\n",
        "  for i in tf.range(10):\n",
        "    x += x\n",
        "    ta = ta.write(i, x)\n",
        "  return ta.stack()\n",
        "\n",
        "f(10.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gVO09og4C_b8"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now revisit the earlier notebooks and try using `tf.function` to speed up your code!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tf.function",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "14-2CMFfiuCftv7Dk9Mwppnx61KMIgXsa",
          "timestamp": 1549585991091
        },
        {
          "file_id": "1-q8UuWhUUTsYTaUhKtPoKsxhFuuxy8Nl",
          "timestamp": 1549055827266
        }
      ],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
