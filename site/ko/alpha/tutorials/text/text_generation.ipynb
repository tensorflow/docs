{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "GCCk8_dHpuNf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcD2nPQvPOFM"
   },
   "source": [
    "# 순환 신경망(RNN, Recurrent Neural Network)을 활용한 문자열 생성\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/text/text_generation.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    TensorFlow.org에서 보기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ko/alpha/tutorials/text/text_generation.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    구글 코랩(Colab)에서 실행하기</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ko/alpha/tutorials/text/text_generation.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    깃허브(GitHub) 소스 보기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 이 문서는 텐서플로 커뮤니티에서 번역했습니다. 커뮤니티 번역 활동의 특성상 정확한 번역과 최신 내용을 반영하기 위해 노력함에도 불구하고 [공식 영문 문서](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb)의 내용과 일치하지 않을 수 있습니다. 이 번역에 개선할 부분이 있다면 [tensorflow/docs](https://github.com/tensorflow/docs) 깃헙 저장소로 풀 리퀘스트를 보내주시기 바랍니다. 문서 번역이나 리뷰에 참여하려면 [docs-ko@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ko)로 메일을 보내주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "이 튜토리얼에서는 문자 기반 순환 신경망(RNN, Recurrent Neural Network)을 사용하여 어떻게 텍스트를 생성하는지 설명합니다.  우리는 Andrej Karpathy의 [순환 신경망의 뛰어난 효율](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)에서 셰익스피어의 작품을 데이터셋으로 작업할 것입니다. 이 데이터(\"Shakespear\")에서 문자 시퀀스가 주어지면, 시퀀스(\"e\")의 다음 문자열을 예측하는 모델을 훈련합니다. 모델을 반복적인 호출은 더 긴 텍스트 시퀀스 생성이 가능합니다.\n",
    "\n",
    "Note: 이 노트북을 더 빠르게 실행하기 위해 GPU 가속을 활성화합니다. 코랩(Colab)에서: Runtime > Change runtime type > Hardware acclerator > GPU* 탭을 선택합니다. 로컬에서 실행하려면 TensorFlow 버전이 1.11 이상이어야 합니다.\n",
    "\n",
    "이 튜토리얼은 [tf.keras](https://www.tensorflow.org/programmers_guide/keras)와 [즉시 실행(eager execution)](https://www.tensorflow.org/programmers_guide/eager)을 활용하여 구현된 실행 가능한 코드가 포함되어 있습니다. 다음은 이 튜토리얼의 30번의 에포크(Epoch)로 훈련된 모델에서 \"Q\" 문자열로 시작될 때의 샘플 출력입니다.\n",
    "\n",
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>\n",
    "\n",
    "문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않습니다. 이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로:\n",
    "\n",
    "* 모델은 문자 기반입니다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르거나 심지어 텍스트의 단위가 단어라는 것도 모릅니다.\n",
    "\n",
    "* 출력의 구조는 대본과 유사합니다. 즉, 텍스트 블록은 대개 화자의 이름으로 시작하고 이 이름들은 모든 데이터셋에서 대문자 씌여 있습니다.\n",
    "\n",
    "* 아래에 설명된 것처럼 이 모델은 작은 텍스트 배치(각 100자)로 훈련받았으며 일관적인 구조로 더 긴 텍스트 시퀀스를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### 텐서플로와 타 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49082.0,
     "status": "ok",
     "timestamp": 1.551671325218E12
    },
    "id": "yG_n40gFzf9s",
    "outputId": "8ce923b9-ccaa-45f4-a8dc-5f53f8a0495a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0-alpha0\n",
      "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190303 tensorflow-estimator-2.0-preview-1.14.0.dev2019030300 tensorflow-gpu==2.0.0-alpha0-2.0.0.dev20190303\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "!pip install tensorflow-gpu==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### 셰익스피어 데이터셋 다운로드\n",
    "\n",
    "본인의 데이터로 변경하여 다음 코드를 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49517.0,
     "status": "ok",
     "timestamp": 1.551671325654E12
    },
    "id": "pD_55cOxLkAb",
    "outputId": "db08f260-286d-4476-f78e-8926ecd05cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### 데이터 읽기\n",
    "\n",
    "먼저, 텍스트를 살펴봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49517.0,
     "status": "ok",
     "timestamp": 1.551671325655E12
    },
    "id": "aavnuByVymwK",
    "outputId": "277c27cb-3dfb-4b05-f682-11d4db376736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# 읽고, py2 compat에 대해 디코딩합니다.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# 텍스트의 길이는 그 안에 있는 문자의 수입니다.\n",
    "print ('텍스트의 길이: {}자'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49517.0,
     "status": "ok",
     "timestamp": 1.551671325656E12
    },
    "id": "Duhg9NrUymwO",
    "outputId": "e05d7036-15c8-4040-c267-8f12ceaf6fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 텍스트의 처음 250자를 살펴봅니다\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49518.0,
     "status": "ok",
     "timestamp": 1.551671325657E12
    },
    "id": "IlCgQBRVymwR",
    "outputId": "dbd961e1-eb1e-4b41-f521-0b721d60fe83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# 파일의 고유 문자수를 출력합니다.\n",
    "vocab = sorted(set(text))\n",
    "print ('고유 문자수 {}개'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 텍스트 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### 텍스트 벡터화\n",
    "\n",
    "훈련 전, 문자들을 수치화할 필요가 있습니다. 두 개의 조회 테이블을 만듭니다: 하나는 문자를 숫자에 매핑하고 다른 하나는 숫자를 문자에 매핑하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# 고유 문자에서 인덱스로 매핑 생성\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "이제 각 문자에 대한 정수 표현을 사용합니다. 문자를 0번 인덱스부터 `고유 문자 길이`까지로 매핑한 것을 기억합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49813.0,
     "status": "ok",
     "timestamp": 1.551671325954E12
    },
    "id": "FYyNlCNXymwY",
    "outputId": "111df4c7-5915-47bd-c0b0-58564a49aafa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49812.0,
     "status": "ok",
     "timestamp": 1.551671325954E12
    },
    "id": "l1VKcQHcymwb",
    "outputId": "ed6fc4ad-d2e2-49f8-d8e6-16f117fee08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트에서 처음 13개의 문자가 숫자로 어떻게 매핑되었는지를 보여줍니다\n",
    "print ('{} ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### 예측 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "주어진 문자열, 혹은 문자 시퀀스에서, 다음 문자열로 어떤 문자가 바람직할까요? 이는 우리가 수행할 모델을 훈련하는 작업입니다. 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 각 시간 단위에서 다음 문자의 출력을 예측합니다.\n",
    "\n",
    "RNN은 이전에 본 요소에 의존하는 내부 상태를 유지하고 있으므로, 이 순간까지 계산된 모든 문자를 감안할 때, 다음 문자는 무엇일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### 훈련 예시 및 대상 생성\n",
    "\n",
    "다음 텍스트를 예제 시퀀스로 나눕니다. 각 입력 시퀀스에는 텍스트의 `seq_length`개의 문자가 포함될 것입니다.\n",
    "\n",
    "각 입력 시퀀스에서, 해당 대상은 오른쪽으로 이동한 한 문자를 제외하고는 동일한 길이의 텍스트를 포함합니다.\n",
    "\n",
    "따라서 텍스트를`seq_length + 1`개의 청크로 나눕니다. 예를 들어,`seq_length`는 4이고 우리의 텍스트는 \"Hello\"입니다. 입력 시퀀스는 \"Hell\"이고 대상 시퀀스는 \"ello\"입니다.\n",
    "\n",
    "이렇게 하기 위해 먼저 텍스트 벡터를 문자 인덱스의 스트림으로 변환하기 위해 `tf.data.Dataset.from_tensor_slices` 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50137.0,
     "status": "ok",
     "timestamp": 1.551671326279E12
    },
    "id": "0UHJDA39zf-O",
    "outputId": "1fadf7e9-47d0-4787-ca5f-6653c55e53d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# 훈련 예시/대상 생성\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "`batch` 메서드는 이 개별 문자들을 원하는 크기의 시퀀스로 쉽게 변환할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50137.0,
     "status": "ok",
     "timestamp": 1.55167132628E12
    },
    "id": "l4hkDU3i7ozi",
    "outputId": "cd0f7a5a-efd8-41bf-8727-072c1f03ad24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "각 시퀀스에서, `map` 메서드를 사용해 각 배치에 간단한 함수를 적용하고 입력 및 대상 텍스트를 복제 및 이동합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "먼저 예시의 입력과 대상 값을 출력합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50138.0,
     "status": "ok",
     "timestamp": 1.551671326283E12
    },
    "id": "GNbw-iR0ymwj",
    "outputId": "7963f3c0-f611-49f7-d489-feb327f303f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('대상 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "이들 벡터의 각 인덱스는 하나의 시간 간격으로 처리됩니다. 시간 단위 0에서 입력에 대해 모델은 \"F\"에 대한 인덱스를 수신하고 \"i\"에 대한 인덱스를 다음 문자로 예측을 시도합니다. 다음 시간 단위에서는 같은 일을 하지만 'RNN'은 현재 입력 문자 외에 이전 단위 컨텍스트를 고려합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50702.0,
     "status": "ok",
     "timestamp": 1.551671326848E12
    },
    "id": "0eBu9WZG84i0",
    "outputId": "79478f3c-f8bb-4cdd-c24d-7f02be247ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  입력: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  예상 출력: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 훈련 배치 생성\n",
    "\n",
    "텍스트를 다루기 쉬운 순서로 분리하기 위해 `tf.data`를 사용했습니다. 그러나 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치 내에서 일괄 처리해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50702.0,
     "status": "ok",
     "timestamp": 1.551671326849E12
    },
    "id": "p2pGotuNzf-S",
    "outputId": "a14cbd1b-3b96-46ea-9480-d39a178e1323"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 배치 크기\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 데이터셋을 섞을 버퍼 크기\n",
    "# (TF 데이터는 무한한 시퀀스와 함께 작동이 가능하도록 설계되었으며,\n",
    "# 따라서 전체 시퀀스를 메모리에 섞으려고 시도하지 않습니다. 대신에,\n",
    "# 요소를 섞는 버퍼를 유지합니다).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "모델을 정의하려면 `tf.keras.Sequential`을 사용합니다. 이 간단한 예제에서는 3개의 층을 사용하여 모델을 정의합니다:\n",
    "\n",
    "* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 수를 매핑하는 훈련 가능한 검색 테이블.\n",
    "* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있습니다.)\n",
    "* `tf.keras.layers.Dense` :`vocab_size`가 출력되는 출력층."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# 문자로 된 단어의 길이\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 임베딩 차원\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN 단위의 개수\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50842.0,
     "status": "ok",
     "timestamp": 1.551671326991E12
    },
    "id": "wwsrpOik5zhv",
    "outputId": "ab6e70f6-0c07-4247-9525-9343148e1fc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0304 03:48:46.706135 140067035297664 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f637273ccf8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "각 문자에 대해 모델은 임베딩을 검색하고, 임베딩을 입력으로 하여 GRU를 1개의 시간단위로 실행하고, 완전연결층을 적용하여 다음 문자의 로그 유사성을 예측하는 로짓을 생성합니다:\n",
    "\n",
    "![모델을 통과하는 데이터의 사진](https://tensorflow.org/tutorials/alpha/text/images/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## 모델 사용\n",
    "\n",
    "이제 모델을 실행하여 예상대로 동작하는지 확인합니다.\n",
    "\n",
    "먼저 출력의 형태를 확인합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55399.0,
     "status": "ok",
     "timestamp": 1.551671331548E12
    },
    "id": "C-_70kKAPrPU",
    "outputId": "8d2e31d8-848b-407f-f8dd-4b9a425de369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "위 예시에서 입력의 시퀀스 길이는 '100'이지만 모델은 임의의 길이 입력에서 실행될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55398.0,
     "status": "ok",
     "timestamp": 1.551671331548E12
    },
    "id": "vPGmAAXmVLGC",
    "outputId": "b877bb47-a0ee-4ecb-b9f7-864089589f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640\n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (64, None, 1024)          5246976\n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625\n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "실제 문자 인덱스를 얻기 위해서는 출력 분포에서 샘플링해야 하는 모델로부터 실제 예측을 얻어야 합니다. 이 분포는 문자 어휘에 대한 로짓에 의해 정의됩니다.\n",
    "\n",
    "Note: 분포의 _argmax_를 취하면 모델이 쉽게 루프에 걸릴 수 있으므로 이 분포에서 _sample_하는 것이 중요합니다.\n",
    "\n",
    "배치의 첫 번째 예시를 시도해 봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "이렇게 하면 각 시간단위에서 다음 문자 인덱스에 대한 예측을 제공합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55399.0,
     "status": "ok",
     "timestamp": 1.551671331551E12
    },
    "id": "YqFMUQc_UFgM",
    "outputId": "402dd14e-51cf-4ba9-fc83-6ec32c63af39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21,  2, 58, 40, 42, 32, 39,  7, 18, 38, 30, 58, 23, 58, 37, 10, 23,\n",
       "       16, 52, 14, 43,  8, 32, 49, 62, 41, 53, 38, 17, 36, 24, 59, 41, 38,\n",
       "        4, 27, 33, 59, 54, 34, 14,  1,  1, 56, 55, 40, 37,  4, 32, 44, 62,\n",
       "       59,  1, 10, 20, 29,  2, 48, 37, 26, 10, 22, 58,  5, 26,  9, 23, 26,\n",
       "       54, 43, 46, 36, 62, 57,  8, 53, 52, 23, 57, 42, 60, 10, 43, 11, 45,\n",
       "       12, 28, 46, 46, 15, 51,  9, 56,  7, 53, 51,  2,  1, 10, 58])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "이 훈련되지 않은 모델에 의해 예측된 텍스트를 보기 위해 이것을 복호화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55400.0,
     "status": "ok",
     "timestamp": 1.551671331552E12
    },
    "id": "xWcFwPwLSo05",
    "outputId": "53e77705-826e-4537-bc4c-cfd225dd9578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " 'to it far before thy time?\\nWarwick is chancellor and the lord of Calais;\\nStern Falconbridge commands'\n",
      "\n",
      "Next Char Predictions:\n",
      " \"I!tbdTa-FZRtKtY:KDnBe.TkxcoZEXLucZ&OUupVB  rqbY&Tfxu :HQ!jYN:Jt'N3KNpehXxs.onKsdv:e;g?PhhCm3r-om! :t\"\n"
     ]
    }
   ],
   "source": [
    "print(\"입력: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"예측된 다음 문자: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "이 관점에서의 문제는 표준 분류 문제로 취급될 수 있습니다. 이전 RNN 상태와 이번 시간단위의 입력으로 다음 문자의 클래스를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### 옵티마이저 장착, 그리고 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "표준 `tf.keras.losses.sparse_softmax_crossentropy`의 손실 함수는 이번 케이스에서 예측의 마지막 차원에 적용되기 때문에 작동합니다.\n",
    "\n",
    "우리 모델은 로짓을 반환하기 때문에 `from_logits` 플래그를 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55398.0,
     "status": "ok",
     "timestamp": 1.551671331552E12
    },
    "id": "4HrXTACTdzY-",
    "outputId": "0d9fbec1-0ad7-4e83-dbc8-aa823b79a165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.174188\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"예측 배열 크기: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"스칼라 손실:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeOXriLcymww"
   },
   "source": [
    "`tf.keras.Model.compile` 메소드를 사용하여 훈련 절차를 설정합니다. 우리는`tf.keras.optimizers.Adam`을 기본 매개변수와 손실 함수와 같이 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### 체크포인트 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "`tf.keras.callbacks.ModelCheckpoint`를 사용하여 훈련 중 체크포인트가 저장되도록 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# 체크포인트가 저장될 디렉토리\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# 체크포인트 파일 이름\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### 훈련 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "합리적인 훈련 시간을 유지하기 위해 모델을 훈련하는 데 10개의 에포크(Epoch)를 사용합니다. 코랩(Colab)에서는 런타임을 GPU로 설정하여 보다 빠르게 훈련할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 365270.0,
     "status": "ok",
     "timestamp": 1.551671641427E12
    },
    "id": "UK-hmKjYVoll",
    "outputId": "8b6348ca-2970-4411-8850-3d17d2dfc7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 31s 183ms/step - loss: 2.7052\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 31s 180ms/step - loss: 2.0039\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 31s 180ms/step - loss: 1.7375\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 31s 179ms/step - loss: 1.5772\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 31s 179ms/step - loss: 1.4772\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 31s 180ms/step - loss: 1.4087\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 31s 179ms/step - loss: 1.3556\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 31s 179ms/step - loss: 1.3095\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 31s 179ms/step - loss: 1.2671\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 31s 180ms/step - loss: 1.2276\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### 최근 체크포인트 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "이 예측 단위를 간단히 유지하기 위해 배치 크기로 1을 사용합니다.\n",
    "\n",
    "RNN 상태가 timestep에서 시간단위로 전달되는 방식이기 때문에 모델은 한 번 빌드된 고정 배치 크기만 허용합니다.\n",
    "\n",
    "다른 'batch_size'로 모델을 실행하기 위해 모델을 다시 빌드하고 체크포인트에서 가중치를 복원해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 365269.0,
     "status": "ok",
     "timestamp": 1.551671641428E12
    },
    "id": "zk2WJ2-XjkGz",
    "outputId": "b476e560-b2ff-4f19-a00a-14b09bb0836f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 365404.0,
     "status": "ok",
     "timestamp": 1.551671641563E12
    },
    "id": "LycQ-ot_jjyu",
    "outputId": "e081c5a5-e905-4002-a22d-68289b53477c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0304 03:54:01.201246 140067035297664 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f636183c7f0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 365404.0,
     "status": "ok",
     "timestamp": 1.551671641564E12
    },
    "id": "71xa6jnYVrAN",
    "outputId": "44b4a32e-07c5-4504-ec8d-ae29332b34d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640\n",
      "_________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM) (1, None, 1024)           5246976\n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625\n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 루프 예측\n",
    "\n",
    "다음 코드 블록은 텍스트를 생성합니다:\n",
    "\n",
    "* 시작 문자열 선택과 순환 신경망(RNN, Recurrent Neural Network) 상태를 초기화하고 생성 할 문자 수를 설정하면 시작됩니다.\n",
    "\n",
    "* 시작 문자열과 순환 신경망(RNN, Recurrent Neural Network) 상태를 사용하여 다음 문자의 예측 분포를 가져옵니다.\n",
    "\n",
    "* 다음, 범주형 분포를 사용하여 예측된 문자의 인덱스를 계산합니다. 이 예측된 문자를 모델에 대한 다음 입력으로 활용합니다.\n",
    "\n",
    "* 모델에 의해 리턴된 RNN 상태는 모델로 피드백되어 이제는 단 한 단어가 아닌 더 많은 컨텍스트를 갖추게 됩니다. 다음 단어를 예측한 후 수정된 RNN 상태가 다시 모델로 피드백되어 이전에 예측된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식입니다.\n",
    "\n",
    "\n",
    "![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/alpha/text/images/text_generation_sampling.png)\n",
    "\n",
    "생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 모방하는지 알 수 있습니다. 적은 훈련 에포크(Epoch)로는 일관된 문장을 형성하는 것은 아직 배우지 못했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
    "\n",
    "  # 생성할 문자의 수\n",
    "  num_generate = 1000\n",
    "\n",
    "  # 시작 문자열을 숫자로 변환(벡터화)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 결과를 저장할 문자열 비우기\n",
    "  text_generated = []\n",
    "\n",
    "  # 도수가 낮으면 더 예측 가능한 텍스트가 됩니다.\n",
    "  # 도수가 높으면 더 의외의 텍스트가 됩니다.\n",
    "  # 최적의 세팅을 찾기 위한 실험\n",
    "  temperature = 1.0\n",
    "\n",
    "  # 여기에서 배치 크기 == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # 배치 차원 제거\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
    "      # 이전 숨겨진 상태와 함께\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 372074.0,
     "status": "ok",
     "timestamp": 1.551671648236E12
    },
    "id": "ktovv0RFhrkn",
    "outputId": "af66d3e5-d868-4345-fe57-53accef36b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: now to have weth hearten sonce,\n",
      "No more than the thing stand perfect your self,\n",
      "Love way come. Up, this is d so do in friends:\n",
      "If I fear e this, I poisple\n",
      "My gracious lusty, born once for readyus disguised:\n",
      "But that a pry; do it sure, thou wert love his cause;\n",
      "My mind is come too!\n",
      "\n",
      "POMPEY:\n",
      "Serve my master's him: he hath extreme over his hand in the\n",
      "where they shall not hear they right for me.\n",
      "\n",
      "PROSSPOLUCETER:\n",
      "I pray you, mistress, I shall be construted\n",
      "With one that you shall that we know it, in this gentleasing earls of daiberkers now\n",
      "he is to look upon this face, which leadens from his master as\n",
      "you should not put what you perciploce backzat of cast,\n",
      "Nor fear it sometime but for a pit\n",
      "a world of Hantua?\n",
      "\n",
      "First Gentleman:\n",
      "That we can fall of bastards my sperial;\n",
      "O, she Go seeming that which I have\n",
      "what enby oar own best injuring them,\n",
      "Or thom I do now, I, in heart is nothing gone,\n",
      "Leatt the bark which was done born.\n",
      "\n",
      "BRUTUS:\n",
      "Both Margaret, he is sword of the house person. If born,\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "결과를 개선하여 더 오래 훈련할 수 있는 가장 쉬운 방법입니다(`EPOCHS = 30`을 시도해 봅시다).\n",
    "\n",
    "또한 다른 시작 문자열을 시험해 보거나 모델의 정확도를 높이기 위해 다른 RNN 레이어를 추가하거나 도수 파라미터를 조정하여 많은 혹은 적은 임의의 예측을 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## 고급: 맞춤식 훈련\n",
    "\n",
    "위의 훈련 절차는 간단하지만 많은 권한을 부여하지는 않습니다.\n",
    "\n",
    "이제 수동으로 모델을 실행하는 방법을 살펴 보았으니 이제 훈련 루프를 해제하고 직접 구현합시다.\n",
    "이는 시작점을 제공해 주는데, 예를 들어 _curriculum learning_을 구현하면 모델의 루프 개방 출력을 안정화시키는 데 도움을 줍니다.\n",
    "\n",
    "기울기 추적을 위해 `tf.GradientTape`을 사용합니다. 이 방법에 대한 자세한 내용은 [eager 실행 가이드](https://www.tensorflow.org/guide/eager)를 참조하십시오.\n",
    "\n",
    "절차는 다음과 같이 동작합니다:\n",
    "\n",
    "* 먼저 RNN 상태를 초기화합니다. 우리는`tf.keras.Model.reset_states` 메소드를 호출하여 이를 수행합니다.\n",
    "\n",
    "* 다음으로 데이터셋(배치별로)를 반복하고 각각에 연관된 *예측*을 계산합니다.\n",
    "\n",
    "* `tf.GradientTape`를 열고 그 컨텍스트에서의 예측과 손실을 계산합니다.\n",
    "\n",
    "* `tf.GradientTape.grads` 메서드를 사용하여 모델 변수에 대한 손실의 기울기를 계산합니다.\n",
    "\n",
    "* 마지막으로 옵티마이저의 `tf.train.Optimizer.apply_gradients` 메서드를 사용하여 이전 단계로 이동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 372211.0,
     "status": "ok",
     "timestamp": 1.551671648374E12
    },
    "id": "_XAm7eCoKULT",
    "outputId": "54455539-bcb7-4e6a-fb41-ba5f5ded466e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0304 03:54:08.030432 140067035297664 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f63635efe80>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUKhnZtMVpoJ"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4kH1o0leVIp"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 675248.0,
     "status": "ok",
     "timestamp": 1.551671951414E12
    },
    "id": "d4tSNwymzf-q",
    "outputId": "85c1ceeb-d189-4b05-e9bf-c66d56673b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.174627780914307\n",
      "Epoch 1 Batch 100 Loss 2.333711862564087\n",
      "Epoch 1 Loss 2.0831\n",
      "Time taken for 1 epoch 15.117910146713257 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.150496244430542\n",
      "Epoch 2 Batch 100 Loss 1.8478351831436157\n",
      "Epoch 2 Loss 1.7348\n",
      "Time taken for 1 epoch 14.401937007904053 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.8013415336608887\n",
      "Epoch 3 Batch 100 Loss 1.6072556972503662\n",
      "Epoch 3 Loss 1.5668\n",
      "Time taken for 1 epoch 14.415359258651733 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.6106206178665161\n",
      "Epoch 4 Batch 100 Loss 1.478020191192627\n",
      "Epoch 4 Loss 1.4673\n",
      "Time taken for 1 epoch 14.300090312957764 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.5047727823257446\n",
      "Epoch 5 Batch 100 Loss 1.3985247611999512\n",
      "Epoch 5 Loss 1.3992\n",
      "Time taken for 1 epoch 14.128910779953003 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.4343167543411255\n",
      "Epoch 6 Batch 100 Loss 1.3426867723464966\n",
      "Epoch 6 Loss 1.3441\n",
      "Time taken for 1 epoch 13.973440170288086 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3767048120498657\n",
      "Epoch 7 Batch 100 Loss 1.297922968864441\n",
      "Epoch 7 Loss 1.2969\n",
      "Time taken for 1 epoch 14.030970573425293 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.3249197006225586\n",
      "Epoch 8 Batch 100 Loss 1.251737117767334\n",
      "Epoch 8 Loss 1.2502\n",
      "Time taken for 1 epoch 14.13421106338501 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.2790530920028687\n",
      "Epoch 9 Batch 100 Loss 1.2029081583023071\n",
      "Epoch 9 Loss 1.2074\n",
      "Time taken for 1 epoch 14.262096643447876 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.2350478172302246\n",
      "Epoch 10 Batch 100 Loss 1.1610674858093262\n",
      "Epoch 10 Loss 1.1558\n",
      "Time taken for 1 epoch 14.261839628219604 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 횟수\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # 모든 에포크(Epoch)의 시작에서 숨겨진 상태를 초기화\n",
    "  # 초기 상태의 hidden은 None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = '에포크 {} 배치 {} 손실 {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # 모든 5 에포크(Epoch)마다(체크포인트) 모델 저장\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('에포크 {} 손실 {:.4f}'.format(epoch+1, loss))\n",
    "  print ('1 에포크 당 {}초 소요\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:tpu_hw_notebook",
    "kind": "private"
   },
   "name": "text_generation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.5 (NGC/TensorFlow 18.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
