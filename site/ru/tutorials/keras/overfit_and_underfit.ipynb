{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "overfit_and_underfit.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "fTFj8ft5dlbS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fTFj8ft5dlbS"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "lzyBOpYMdp3F",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "m_x4KfSJ7Vt7",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C9HmC2T4ld5B"
      },
      "source": [
        "# Explore overfit and underfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kRTxFhXAlnl1"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />Смотрите на TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ru/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Запустите в Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ru/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />Изучайте код на GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/ru/tutorials/keras/overfit_and_underfit.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Скачайте ноутбук</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZwaVqMxhh9_",
        "colab_type": "text"
      },
      "source": [
        "Note: Вся информация в этом разделе переведена с помощью русскоговорящего Tensorflow сообщества на общественных началах. Поскольку этот перевод не является официальным, мы не гарантируем что он на 100% аккуратен и соответствует [официальной документации на английском языке](https://www.tensorflow.org/?hl=en). Если у вас есть предложение как исправить этот перевод, мы будем очень рады увидеть pull request в [tensorflow/docs](https://github.com/tensorflow/docs) репозиторий GitHub. Если вы хотите помочь сделать документацию по Tensorflow лучше (сделать сам перевод или проверить перевод подготовленный кем-то другим), напишите нам на [docs-ru@tensorflow.org list](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ru)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19rPukKZsPG6"
      },
      "source": [
        "Как всегда, код в этом примере будет использовать API `tf.keras`, о котором вы можете узнать больше в [руководстве TensorFlow Keras](https://www.tensorflow.org/guide/keras).\n",
        "\n",
        "В обоих предыдщих примерах с классификацией обзоров фильмов и предсказанием эффективности расхода топлива, мы увидели, что точность нашей модели на проверочных данных достигает пика после обучения за некоторое количество эпох, а затем начинает снижаться.\n",
        "\n",
        "Другими словами, наша модель *переобучилась* на тренировочных данных. Важно научиться работать с переобученностью. Хотя часто возможно достичь высокой точности на *обучающей выборке*, на самом деле мы хотим построить модель которая хорошо обобщается на *тестовой выборке* (данных которые модель не видела ранее).\n",
        "\n",
        "Обратным случаем переобучения является *недообучение*. Недообучение возникает когда еще есть возможность для улучшения модели на тестовых данных. Это может случитья по ряду причин: модель недостаточно сильная, с избыточной регуляризацией или просто недостаточно долго обучалась. Это значит, что сеть не изучила релевантные паттерны в обучающей выборке.\n",
        "\n",
        "Если ты будешь обучать модель слишком долго, модель начнет переобучаться и настроится на паттерны тренировочных данных которые не обобщаются на тестовые данные. Нам нужно найти баланс. Понимание того, как обучать модель за подходящее количество эпох, как мы выясним ниже - очень полезный навык.\n",
        "\n",
        "Лучшее решение для предотвращения переобученности - использовать больше тренировочных данных. Модель обученная на большем количестве данных естественным образом обобщает лучше. Когда это более невозможно, следующее решение - использовать техники наподобие регуляризации. Они накладывают ограничения на количество и тип информации которую ваша модель может хранить. Если нейросеть может запомнить только небольшое число паттернов, то процесс оптимизации заставит ее сфокусироваться на наиболее заметных паттернах, у которых более высокий шанс обобщения.\n",
        "\n",
        "В этом уроке мы познакомимся с двумя распространенными техниками регуляризации - *регуляризацией весов* и *исключением (dropout)* и используем их для того, чтобы улучшить нашу модель классификации обзоров фильмов из IMDB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pZ8A2liqvgk",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version существует только в Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cweoTiruj8O"
      },
      "source": [
        "## Загрузите датасет IMDB\n",
        "\n",
        "Вместо использования вложения (embedding) как в предыдущем уроке, здесь мы используем multi-hot encode предложений. Эта модель быстро переобучится на тренировочных данных. Мы посмотрим как произойдет переобучение и как его предотвратить.\n",
        "\n",
        "Multi-hot-encoding наших списков означет их преобразование в вектора из 0 и 1. Конкретнее это  значит что например последовательность `[3, 5]` преобразуется в 10 000-мерный вектор, который будет состоять полностью из нулей за исключением индексов 3 и 5 которые будут единицами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QpzE4iqZtJly",
        "colab": {}
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "def multi_hot_sequences(sequences, dimension):\n",
        "    # Создадим нулевую матрицу размерности (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, word_indices in enumerate(sequences):\n",
        "        results[i, word_indices] = 1.0  # приравняем требуемые индексы results[i] к 1\n",
        "    return results\n",
        "\n",
        "\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzWVeXe3NBTn"
      },
      "source": [
        "Давайте посмотрим на один из получившихся multi-hot векторов. Индексы слов были отсортированы по частоте поэтому ожидаемо много значений 1 возле нулевого индекса, что мы и видим на этом графике:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71kr5rG4LkGM",
        "colab": {}
      },
      "source": [
        "plt.plot(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lglk41MwvU5o"
      },
      "source": [
        "## Продемонстрируем переобучение\n",
        "\n",
        "Простейший способ предотвратить переобучение это сократить размер модели, т.е. количество обучаемых параметров модели (которые определяются числом слоев и элементов в каждом слое). В глубоком обучении количество обучаемых параметров модели часто называют \"емкостью\" модели. Интуитивно, модель с большим количеством параметров будет иметь большую \"запоминающую емкость\" и поэтому легко сможет выучить идеальный словарь - как отображение между обучающими примерами и их целевыми значениями, отображение безо всякой обобщающей силы. Но это будет бесполезно при прогнозировании на новых, ранее не виденных данных.\n",
        "\n",
        "Всегда имейте это ввиду: модели глубокого обучения хорошо настраиваются на тренировочных данных, но настоящим вызовом является обобщение, не обучение.\n",
        "\n",
        "С другой стороны, если нейросеть имеет ограниченные ресурсы ззапоминания, то она не сможет выучить отображение так легко. Для минимизации функции потерь модель вынуждена выучить только сжатые представления у которых больше предсказательной силы. В то же время, если вы сделаете вашу модель слишком маленькой, она с трудом подстроится под тренировочный сет. Существует баланс между \"слишком большой емкостью\" и \"недостаточной емкостью\".\n",
        "\n",
        "К сожалению, не существует магической формулы, чтобы определить правильный размер или архитектуру модели, говоря о количестве слоев или размере каждого слоя. Вам необходимо поэкспериментировать с использованием разных архитектур модели.\n",
        "\n",
        "Чтобы найди подходящий размер модели лучше начать с относительно небольшого количества слоев и параметров, затем начать увеличивать размер слоев или добавлять новые до тех пор, пока вы не увидите уменьшение отдачи функции ошибок на проверочных данных. Давай попробуем это на примере нашей сети для классификации обзоров фильмов.\n",
        "\n",
        "Мы построим простую модель используя только ```Dense``` слои в качестве базовой, затем создадим меньшую и большую версии модели и сравним их."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "### Создайте базовую модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QKgdXPx9usBa",
        "colab": {}
      },
      "source": [
        "baseline_model = keras.Sequential([\n",
        "    # Параметр `input_shape` нужен только для того, чтобы заработал `.summary`.\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "baseline_model.compile(optimizer='adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "baseline_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LqG3MXF5xSjR",
        "colab": {}
      },
      "source": [
        "baseline_history = baseline_model.fit(train_data,\n",
        "                                      train_labels,\n",
        "                                      epochs=20,\n",
        "                                      batch_size=512,\n",
        "                                      validation_data=(test_data, test_labels),\n",
        "                                      verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L-DGRBbGxI6G"
      },
      "source": [
        "### Создайте меньшую модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrfoVQheYSO5"
      },
      "source": [
        "Давайте построим модель с меньшим количеством скрытых нейронов чтобы сравнить ее с базовой моделью, которую мы только создали:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jksi-XtaxDAh",
        "colab": {}
      },
      "source": [
        "smaller_model = keras.Sequential([\n",
        "    keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(4, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "smaller_model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "smaller_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jbngCZliYdma"
      },
      "source": [
        "И обучим модель используя те же данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ofn1AwDhx-Fe",
        "colab": {}
      },
      "source": [
        "smaller_history = smaller_model.fit(train_data,\n",
        "                                    train_labels,\n",
        "                                    epochs=20,\n",
        "                                    batch_size=512,\n",
        "                                    validation_data=(test_data, test_labels),\n",
        "                                    verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vIPuf23FFaVn"
      },
      "source": [
        "### Создайте большую модель\n",
        "\n",
        "В качестве упражнения вы можете построить еще большую модель и увидеть как быстро она начнет переобучаться. Далее давайте сравним с эталоном нейросеть которая имеет намного большую емкость чем того требует задача:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ghQwwqwqvQM9",
        "colab": {}
      },
      "source": [
        "bigger_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "bigger_model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy','binary_crossentropy'])\n",
        "\n",
        "bigger_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D-d-i5DaYmr7"
      },
      "source": [
        "И опять обучим модель используя те же данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U1A99dhqvepf",
        "colab": {}
      },
      "source": [
        "bigger_history = bigger_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### Постройте графики потерь на тренировочных и проверочных данных\n",
        "\n",
        "<!--TODO(markdaoust): This should be a one-liner with tensorboard -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "Непрерывные линии показывают потери во время обучения, а прерывистые - во время проверки (помни - меньшие потери на проверочных данных указывают на лучшую модель). В нашем случае самая маленькая модель начинает переобучаться позже, чем основная (после 6 эпох вместо 4) и ее показатели ухудшаются гораздо медленее после переобучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0XmKDtOWzOpk",
        "colab": {}
      },
      "source": [
        "def plot_history(histories, key='binary_crossentropy'):\n",
        "  plt.figure(figsize=(16,10))\n",
        "\n",
        "  for name, history in histories:\n",
        "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key.replace('_',' ').title())\n",
        "  plt.legend()\n",
        "\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "\n",
        "plot_history([('baseline', baseline_history),\n",
        "              ('smaller', smaller_history),\n",
        "              ('bigger', bigger_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bi6hBhdnSfjA"
      },
      "source": [
        "Обратите внимание, что большая сеть начинает переобучаться почти сразу же после первой эпохи, и переобучение происходит гораздо быстрее. Чем больше емкость модели, тем легче она смоделирует тренировочные данные (и мы получим низкое значение потерь на тренировочных данных). Но в таком случае она будет более чувствительна к переобучению: разница потерь между обучением и проверкой будет очень велика."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ASdv7nsgEFhx"
      },
      "source": [
        "## Стратегии предотвращения переобучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4rHoVWcswFLa"
      },
      "source": [
        "### Добавить регуляризацию весов\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kRxWepNawbBK"
      },
      "source": [
        "Вам может быть знаком принцип бритвы Оккама: из двух толкований некоторого явления, правильным скорее всего является самое \"простое\" - то, которое содержит меньше всего предположений. Этот принцип также применим к моделям, обучемым при помощи нейронных сетей: если у наших данных и сетевой архитектуры существует несколько наборов значений весов (несколько моделей) которые могут объяснить данные и более простые модели переобучаются реже, чем сложные.\n",
        "\n",
        "В этом контексте \"простая модель\" это модель в которой распределение значений параметров имеет меньшую энтропию (или модель с меньшим количеством параметров, как та которую мы строили выше). Таким образом, для предотвращение переобучения часто используется ограничение сложности сети путем принуждения ее коэфицентов принимать только небольшие значения, что делает распределение весов более \"регулярным\". Этот метод называется \"регуляризация весов\": к функции потерь нашей сети мы добавляем штраф (или cost, стоимость) за использование больших весов. Регуляризация бывает двух видов:\n",
        "\n",
        "* [L1 регуляризация](https://developers.google.com/machine-learning/glossary/#L1_regularization), где добавляемый штраф пропорционален абсолютным значениям коэффициентов весов (т.е. то что называется \"L1 нормой\" весов).\n",
        "\n",
        "* [L2 регуляризация](https://developers.google.com/machine-learning/glossary/#L2_regularization), где добавляемый штраф пропорционален квадрату значений коэффициентов весов (т.е. то, что  называется квадратом  \"L2 нормы\" весов). L2 регуляризацию также называют сокращением весов в контексте нейросетей. Не дайте разным названиям запутать себя: сокращение весов математически ровно то же самое что и L2 регуляризация.\n",
        "\n",
        "L1 регуляризация вводит разреженность обнуляя некотороые из ваших весовых параметров. L2 регуляризация оштрафует весовые параметры не делая их разреженными - это одна из причин, почему L2 более распространена.\n",
        "\n",
        "В `tf.keras` регуляризация весов добавляется передачей экземпляров регуляризатора слоям в качестве аргумента. Добавим сейчас L2 регуляризатор весов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFGmcwduwVyQ",
        "colab": {}
      },
      "source": [
        "l2_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "l2_model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy', 'binary_crossentropy'])\n",
        "\n",
        "l2_model_history = l2_model.fit(train_data, train_labels,\n",
        "                                epochs=20,\n",
        "                                batch_size=512,\n",
        "                                validation_data=(test_data, test_labels),\n",
        "                                verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bUUHoXb7w-_C"
      },
      "source": [
        "```l2(0.001)``` значит что каждый коэффициент в матрице весов слоя добавит ```0.001 * weight_coefficient_value**2``` к значению потерь нейросети. Заметьте, что поскольку этот штраф доавляется только во время обучения, потери сети во время этой стадии будут гораздо выше чем во время теста.\n",
        "\n",
        "Так выглядит влияние регуляризации L2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7wkfLyxBZdh_",
        "colab": {}
      },
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('l2', l2_model_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx1YHMsVxWjP"
      },
      "source": [
        "Как вы можете видеть L2 регуляризованная модель стала более устойчивой к переобучению чем базовая модель, несмотря на то что обе модели имеют одинаковое количество параметров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HmnBNOOVxiG8"
      },
      "source": [
        "### Добавьте дропаут\n",
        "\n",
        "Дропаут(исключение) одна из наиболее эффективных и часто используемых техник регуляризации нейросетей разработанная Джеффом Хинтоном и его студентами в университете Торонто. Примененный к слою Dropout состоит из некоторого количества случайно \"исключенных\" (т.е. приравненных к нулю) во время обучения выходных параметров слоя. Допустим что наш слой возвращает вектор [0.2, 0.5, 1.3, 0.8, 1.1] для некоторых входных данных при обучении; после применения дропаута, в этом векторе появится несколько нулевых значений распределенных случайным образом, например [0, 0.5,\n",
        "1.3, 0, 1.1]. \"Коэффициент дропаута(dropout rate)\" это доля признаков которые будут обнулены; его обычно устанавливают между 0.2 и 0.5. Во время теста дропаут не используется, вместо этого выходные данные слоев масштабируются на коэффициент равный коэффициенту дропаута, чтобы сбалансировать тот факт, что во время проверки активно больше нейронов чем во время обучения.\n",
        "\n",
        "В tf.keras можно ввести дропаут с помощью слоя Dropout, который применяется к выходным данным предыдущего слоя.\n",
        "\n",
        "Давайте применим два слоя Dropout к нашей нейросети IMDB и посмотрим насколько хорошо она сократит переобучение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFEYvtrHxSWS",
        "colab": {}
      },
      "source": [
        "dpt_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "dpt_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy','binary_crossentropy'])\n",
        "\n",
        "dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPZqwVchx5xp",
        "colab": {}
      },
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('dropout', dpt_model_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gjfnkEeQyAFG"
      },
      "source": [
        "Добавление дропаута явно улучает базовую модель.\n",
        "\n",
        "Подведем итоги - вот самые основные способы предотвращения переобучения нейросетей:\n",
        "\n",
        "* Использовать больше данных для обучения.\n",
        "* Уменьшить емкость сети.\n",
        "* Использовать регуляризацию весов.\n",
        "* Добавить дропаут.\n",
        "\n",
        "Два важных подхода которые не были рассмотрены в данном руководстве это аугментация данных (data-augmentation) и батч-нормализация (batch normalization)."
      ]
    }
  ]
}