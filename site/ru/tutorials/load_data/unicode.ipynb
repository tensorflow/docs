{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unicode.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "oL9KopJirB2g"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oL9KopJirB2g"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "SKaX3Hd3ra6C",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AXH1bmUctMld"
      },
      "source": [
        "# –Æ–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/unicode\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/unicode.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/unicode.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/load_data/unicode.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LrHJrKYis06U"
      },
      "source": [
        "## –í–≤–µ–¥–µ–Ω–∏–µ\n",
        "\n",
        "–ú–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —è–∑—ã–∫–∏, —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –¥–µ–ª–æ —Å —Ä–∞–∑–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏ –∏ —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ —Å–∏–º–≤–æ–ª–æ–≤. * Unicode * - —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤. –ö–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ü–µ–ª–æ–≥–æ —á–∏—Å–ª–∞ [code point] (https://en.wikipedia.org/wiki/Code_point) –º–µ–∂–¥—É `0` –∏` 0x10FFFF`. *–Æ–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∞* - —ç—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –Ω—É–ª—è –∏–ª–∏ –±–æ–ª–µ–µ —Ç–∞–∫–∏—Ö —é–Ω–∏–∫–æ–¥-—Å–∏–º–≤–æ–ª–æ–≤.\n",
        "\n",
        "–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —é–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏ –≤ Tensorflow –∏ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å –∏–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è —é–Ω–∏–∫–æ–¥–æ–≤—Å–∫–∏–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–≤–æ–π. –û–Ω–∞ –≤—ã–¥–µ–ª—è–µ—Ç —é–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏ –≤ —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OIKHl5Lvn4gh",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç–æ–ª—å–∫–æ –≤ Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n-LkcI-vtWNj"
      },
      "source": [
        "## –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö `tf.string`\n",
        "\n",
        "–ë–∞–∑–æ–≤—ã–π TensorFlow `tf.string` `dtype` –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∞–º —Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã –±–∞–π—Ç-—Å—Ç—Ä–æ–∫.\n",
        "–Æ–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ utf-8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3yo-Qv6ntaFr",
        "colab": {}
      },
      "source": [
        "tf.constant(u\"–°–ø–∞—Å–∏–±–æ üòä\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kA1ziG2tyCT"
      },
      "source": [
        "–¢–µ–Ω–∑–æ—Ä `tf.string` –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –±–∞–π—Ç-—Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω—ã –ø–æ—Å–∫–æ–ª—å–∫—É –±–∞–π—Ç-—Å—Ç—Ä–æ–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã. –î–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω–∞ –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä–∞.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eyINCmTztyyS",
        "colab": {}
      },
      "source": [
        "tf.constant([u\"–î–æ–±—Ä–æ\", u\"–ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!\"]).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jsMPnjb6UDJ1"
      },
      "source": [
        "–ó–∞–º–µ—á–∞–Ω–∏–µ: –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ python –ø—Ä–∏ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —é–Ω–∏–∫–æ–¥–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É v2 –∏ v3. –í v2, —é–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏ –æ—Ç–º–µ—á–µ–Ω—ã –ø—Ä–µ—Ñ–∏–∫—Å–æ–º \"u\", –∫–∞–∫ –∏ –≤—ã—à–µ. –í v3, —Å—Ç—Ä–æ–∫–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –≤ —é–Ω–∏–∫–æ–¥–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hUFZ7B1Lk-uj"
      },
      "source": [
        "## –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Æ–Ω–∏–∫–æ–¥–∞\n",
        "\n",
        "–ï—Å—Ç—å –¥–≤–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —é–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫ –≤ TensorFlow:\n",
        "\n",
        "* `string` —Å–∫–∞–ª—è—Ä ‚Äî –≥–¥–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —é–Ω–∏–∫–æ–¥-—Å–∏–º–≤–æ–ª–æ–≤ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º [–Ω–∞–±–æ—Ä–∞ —Å–∏–º–≤–æ–ª–æ–≤](https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B1%D0%BE%D1%80_%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%BE%D0%B2).\n",
        "* `int32` –≤–µ–∫—Ç–æ—Ä ‚Äî –≥–¥–µ –∫–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —é–Ω–∏–∫–æ–¥-—Å–∏–º–≤–æ–ª.\n",
        "\n",
        "–ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–µ–¥—É—é—â–∏–µ —Ç—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —é–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫—É `\"ËØ≠Ë®ÄÂ§ÑÁêÜ\"` (—á—Ç–æ –∑–Ω–∞—á–∏—Ç \"–æ–±—Ä–∞–±–æ—Ç–∫–∞ —è–∑—ã–∫–∞\" –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cjQIkfJWvC_u",
        "colab": {}
      },
      "source": [
        "# –Æ–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–∞–∫ UTF-8 –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–∫–∞–ª—è—Ä—ã.\n",
        "text_utf8 = tf.constant(u\"ËØ≠Ë®ÄÂ§ÑÁêÜ\")\n",
        "text_utf8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yQqcUECcvF2r",
        "colab": {}
      },
      "source": [
        "# –Æ–Ω–∏–∫–æ–¥-—Å—Ç—Ä–æ–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–∞–∫ UTF-16-BE –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–∫–∞–ª—è—Ä—ã.\n",
        "text_utf16be = tf.constant(u\"ËØ≠Ë®ÄÂ§ÑÁêÜ\".encode(\"UTF-16-BE\"))\n",
        "text_utf16be"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExdBr1t7vMuS",
        "colab": {}
      },
      "source": [
        "# –Æ–Ω–∏–∫–æ–¥ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä—ã —é–Ω–∏–∫–æ–¥-—Å–∏–º–≤–æ–ª–æ–≤.\n",
        "text_chars = tf.constant([ord(char) for char in u\"ËØ≠Ë®ÄÂ§ÑÁêÜ\"])\n",
        "text_chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B8czv4JNpBnZ"
      },
      "source": [
        "### Converting between representations\n",
        "\n",
        "TensorFlow provides operations to convert between these different representations:\n",
        "\n",
        "* `tf.strings.unicode_decode`: Converts an encoded string scalar to a vector of code points.\n",
        "* `tf.strings.unicode_encode`: Converts a vector of code points to an encoded string scalar.\n",
        "* `tf.strings.unicode_transcode`: Converts an encoded string scalar to a different encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qb-UQ_oLpAJg",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_decode(text_utf8,\n",
        "                          input_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEBUcunnp-9n",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_encode(text_chars,\n",
        "                          output_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MLhWcLZrph-",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_transcode(text_utf8,\n",
        "                             input_encoding='UTF8',\n",
        "                             output_encoding='UTF-16-BE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QVeLeVohqN7I"
      },
      "source": [
        "### Batch dimensions\n",
        "\n",
        "When decoding multiple strings, the number of characters in each string may not be equal.  The return result is a [`tf.RaggedTensor`](../../guide/ragged_tensors.ipynb), where the length of the innermost dimension varies depending on the number of characters in each string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N2jVzPymr_Mm",
        "colab": {}
      },
      "source": [
        "# A batch of Unicode strings, each represented as a UTF8-encoded string.\n",
        "batch_utf8 = [s.encode('UTF-8') for s in\n",
        "              [u'h√Éllo',  u'What is the weather tomorrow',  u'G√∂√∂dnight', u'üòä']]\n",
        "batch_chars_ragged = tf.strings.unicode_decode(batch_utf8,\n",
        "                                               input_encoding='UTF-8')\n",
        "for sentence_chars in batch_chars_ragged.to_list():\n",
        "  print(sentence_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iRh3n1hPsJ9v"
      },
      "source": [
        "You can use this `tf.RaggedTensor` directly, or convert it to a dense `tf.Tensor` with padding or a `tf.SparseTensor` using the methods `tf.RaggedTensor.to_tensor` and `tf.RaggedTensor.to_sparse`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yz17yeSMsUid",
        "colab": {}
      },
      "source": [
        "batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1)\n",
        "print(batch_chars_padded.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBjsPQp3rhfm",
        "colab": {}
      },
      "source": [
        "batch_chars_sparse = batch_chars_ragged.to_sparse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCCkZh-nwlbL"
      },
      "source": [
        "When encoding multiple strings with the same lengths, a `tf.Tensor` may be used as input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_lP62YUAwjK9",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_encode([[99, 97, 116], [100, 111, 103], [ 99, 111, 119]],\n",
        "                          output_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w58CMRg9tamW"
      },
      "source": [
        "When encoding multiple strings with varyling length, a `tf.RaggedTensor` should be used as input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7GtOtrltaMl",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_encode(batch_chars_ragged, output_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T2Nh5Aj9xob3"
      },
      "source": [
        "If you have a tensor with multiple strings in padded or sparse format, then convert it to a `tf.RaggedTensor` before calling `unicode_encode`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2bYCYl0u-Ue",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_encode(\n",
        "    tf.RaggedTensor.from_sparse(batch_chars_sparse),\n",
        "    output_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UlV2znh_u_zm",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_encode(\n",
        "    tf.RaggedTensor.from_tensor(batch_chars_padded, padding=-1),\n",
        "    output_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hQOOGkscvDpc"
      },
      "source": [
        "## Unicode operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NkmtsA_yvMB0"
      },
      "source": [
        "### Character length\n",
        "\n",
        "The `tf.strings.length` operation has a parameter `unit`, which indicates how lengths should be computed.  `unit` defaults to `\"BYTE\"`, but it can be set to other values, such as `\"UTF8_CHAR\"` or `\"UTF16_CHAR\"`, to determine the number of Unicode codepoints in each encoded `string`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ZzMe59mvLHr",
        "colab": {}
      },
      "source": [
        "# Note that the final character takes up 4 bytes in UTF8.\n",
        "thanks = u'Thanks üòä'.encode('UTF-8')\n",
        "num_bytes = tf.strings.length(thanks).numpy()\n",
        "num_chars = tf.strings.length(thanks, unit='UTF8_CHAR').numpy()\n",
        "print('{} bytes; {} UTF-8 characters'.format(num_bytes, num_chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fHG85gxlvVU0"
      },
      "source": [
        "### Character substrings\n",
        "\n",
        "Similarly, the `tf.strings.substr` operation accepts the \"`unit`\" parameter, and uses it to determine what kind of offsets the \"`pos`\" and \"`len`\" paremeters contain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WlWRLV-4xWYq",
        "colab": {}
      },
      "source": [
        "# default: unit='BYTE'. With len=1, we return a single byte.\n",
        "tf.strings.substr(thanks, pos=7, len=1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JfNUVDPwxkCS",
        "colab": {}
      },
      "source": [
        "# Specifying unit='UTF8_CHAR', we return a single character, which in this case\n",
        "# is 4 bytes.\n",
        "print(tf.strings.substr(thanks, pos=7, len=1, unit='UTF8_CHAR').numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zJUEsVSyeIa3"
      },
      "source": [
        "### Split Unicode strings\n",
        "\n",
        "The `tf.strings.unicode_split` operation splits unicode strings into substrings of individual characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dDjkh5G1ejMt",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_split(thanks, 'UTF-8').numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HQqEEZEbdG9O"
      },
      "source": [
        "### Byte offsets for characters\n",
        "\n",
        "To align the character tensor generated by `tf.strings.unicode_decode` with the original string, it's useful to know the offset for where each character begins.  The method `tf.strings.unicode_decode_with_offsets` is similar to `unicode_decode`, except that it returns a second tensor containing the start offset of each character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cug7cmwYdowd",
        "colab": {}
      },
      "source": [
        "codepoints, offsets = tf.strings.unicode_decode_with_offsets(u\"üéàüéâüéä\", 'UTF-8')\n",
        "\n",
        "for (codepoint, offset) in zip(codepoints.numpy(), offsets.numpy()):\n",
        "  print(\"At byte offset {}: codepoint {}\".format(offset, codepoint))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ZnCNxOvx66T"
      },
      "source": [
        "## Unicode scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRRHqkqNyGZ6"
      },
      "source": [
        "Each Unicode code point belongs to a single collection of codepoints known as a [script](https://en.wikipedia.org/wiki/Script_%28Unicode%29) .  A character's script is helpful in determining which language the character might be in. For example, knowing that '–ë' is in Cyrillic script indicates that modern text containing that character is likely from a Slavic language such as Russian or Ukrainian.\n",
        "\n",
        "TensorFlow provides the `tf.strings.unicode_script` operation to determine which script a given codepoint uses. The script codes are `int32` values corresponding to [International Components for\n",
        "Unicode](http://site.icu-project.org/home) (ICU) [`UScriptCode`](http://icu-project.org/apiref/icu4c/uscript_8h.html) values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K7DeYHrRyFPy",
        "colab": {}
      },
      "source": [
        "uscript = tf.strings.unicode_script([33464, 1041])  # ['Ëä∏', '–ë']\n",
        "\n",
        "print(uscript.numpy())  # [17, 8] == [USCRIPT_HAN, USCRIPT_CYRILLIC]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2fW992a1lIY6"
      },
      "source": [
        "The `tf.strings.unicode_script` operation can also be applied to multidimensional `tf.Tensor`s or `tf.RaggedTensor`s of codepoints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uR7b8meLlFnp",
        "colab": {}
      },
      "source": [
        "print(tf.strings.unicode_script(batch_chars_ragged))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mx7HEFpBzEsB"
      },
      "source": [
        "## Example: Simple segmentation\n",
        "\n",
        "Segmentation is the task of splitting text into word-like units. This is often easy when space characters are used to separate words, but some languages (like Chinese and Japanese) do not use spaces, and some languages (like German) contain long compounds that must be split in order to analyze their meaning. In web text, different languages and scripts are frequently mixed together, as in \"NYÊ†™‰æ°\" (New York Stock Exchange).\n",
        "\n",
        "We can perform very rough segmentation (without implementing any ML models) by using changes in script to approximate word boundaries. This will work for strings like the \"NYÊ†™‰æ°\" example above. It will also work for most languages that use spaces, as the space characters of various scripts are all classified as USCRIPT_COMMON, a special script code that differs from that of any actual text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grsvFiC4BoPb",
        "colab": {}
      },
      "source": [
        "# dtype: string; shape: [num_sentences]\n",
        "#\n",
        "# The sentences to process.  Edit this line to try out different inputs!\n",
        "sentence_texts = [u'Hello, world.', u'‰∏ñÁïå„Åì„Çì„Å´„Å°„ÅØ']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CapnbShuGU8i"
      },
      "source": [
        "First, we decode the sentences into character codepoints, and find the script identifeir for each character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ReQVcDQh1MB8",
        "colab": {}
      },
      "source": [
        "# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]\n",
        "#\n",
        "# sentence_char_codepoint[i, j] is the codepoint for the j'th character in\n",
        "# the i'th sentence.\n",
        "sentence_char_codepoint = tf.strings.unicode_decode(sentence_texts, 'UTF-8')\n",
        "print(sentence_char_codepoint)\n",
        "\n",
        "# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]\n",
        "#\n",
        "# sentence_char_scripts[i, j] is the unicode script of the j'th character in\n",
        "# the i'th sentence.\n",
        "sentence_char_script = tf.strings.unicode_script(sentence_char_codepoint)\n",
        "print(sentence_char_script)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O2fapF5UGcUc"
      },
      "source": [
        "Next, we use those script identifiers to determine where word boundaries should be added.  We add a word boundary at the beginning of each sentence, and for each character whose script differs from the previous character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7v5W6MOr1Rlc",
        "colab": {}
      },
      "source": [
        "# dtype: bool; shape: [num_sentences, (num_chars_per_sentence)]\n",
        "#\n",
        "# sentence_char_starts_word[i, j] is True if the j'th character in the i'th\n",
        "# sentence is the start of a word.\n",
        "sentence_char_starts_word = tf.concat(\n",
        "    [tf.fill([sentence_char_script.nrows(), 1], True),\n",
        "     tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])],\n",
        "    axis=1)\n",
        "\n",
        "# dtype: int64; shape: [num_words]\n",
        "#\n",
        "# word_starts[i] is the index of the character that starts the i'th word (in\n",
        "# the flattened list of characters from all sentences).\n",
        "word_starts = tf.squeeze(tf.where(sentence_char_starts_word.values), axis=1)\n",
        "print(word_starts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LAwh-1QkGuC9"
      },
      "source": [
        "We can then use those start offsets to build a `RaggedTensor` containing the list of words from all batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bNiA1O_eBBCL",
        "colab": {}
      },
      "source": [
        "# dtype: int32; shape: [num_words, (num_chars_per_word)]\n",
        "#\n",
        "# word_char_codepoint[i, j] is the codepoint for the j'th character in the\n",
        "# i'th word.\n",
        "word_char_codepoint = tf.RaggedTensor.from_row_starts(\n",
        "    values=sentence_char_codepoint.values,\n",
        "    row_starts=word_starts)\n",
        "print(word_char_codepoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "66a2ZnYmG2ao"
      },
      "source": [
        "And finally, we can segment the word codepoints `RaggedTensor` back into sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCfwcqLSEjZb",
        "colab": {}
      },
      "source": [
        "# dtype: int64; shape: [num_sentences]\n",
        "#\n",
        "# sentence_num_words[i] is the number of words in the i'th sentence.\n",
        "sentence_num_words = tf.reduce_sum(\n",
        "    tf.cast(sentence_char_starts_word, tf.int64),\n",
        "    axis=1)\n",
        "\n",
        "# dtype: int32; shape: [num_sentences, (num_words_per_sentence), (num_chars_per_word)]\n",
        "#\n",
        "# sentence_word_char_codepoint[i, j, k] is the codepoint for the k'th character\n",
        "# in the j'th word in the i'th sentence.\n",
        "sentence_word_char_codepoint = tf.RaggedTensor.from_row_lengths(\n",
        "    values=word_char_codepoint,\n",
        "    row_lengths=sentence_num_words)\n",
        "print(sentence_word_char_codepoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xWaX8WcbHyqY"
      },
      "source": [
        "To make the final result easier to read, we can encode it back into UTF-8 strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HSivquOgFr3C",
        "colab": {}
      },
      "source": [
        "tf.strings.unicode_encode(sentence_word_char_codepoint, 'UTF-8').to_list()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}